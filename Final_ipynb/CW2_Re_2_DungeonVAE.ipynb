{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Data preparation"
      ],
      "metadata": {
        "id": "bkzqwaTr0d0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import and verify"
      ],
      "metadata": {
        "id": "u_VzeTGO0pTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming dungeons_dataset.npy is located in the current working directory\n",
        "dataset_file = 'dungeons_dataset.npy'\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    dungeons_data = np.load(dataset_file)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Dataset file '{dataset_file}' not found.\")\n",
        "    # Handle the case where the file is not found\n",
        "    dungeons_data = None\n",
        "\n",
        "# Verify the dataset shape and contents\n",
        "if dungeons_data is not None:\n",
        "    # Check the shape of the dataset\n",
        "    dataset_shape = dungeons_data.shape\n",
        "    print(f\"Dataset shape: {dataset_shape}\")\n",
        "\n",
        "    # Check if any record is missing (e.g., NaN values or empty entries)\n",
        "    if not np.any(np.isnan(dungeons_data)) and not np.any(np.isinf(dungeons_data)):\n",
        "        print(\"No missing records found.\")\n",
        "    else:\n",
        "        print(\"There are missing records in the dataset.\")\n",
        "else:\n",
        "    print(\"Dataset not loaded, cannot perform further verification.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1fMrYgs0xJS",
        "outputId": "caa7bb93-fd53-4e7c-d023-4ae58e342156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Dataset shape: (253, 8, 8)\n",
            "No missing records found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Augment the data"
      ],
      "metadata": {
        "id": "xVH5fDjL1RQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import rotate\n",
        "\n",
        "# Assuming dungeons_dataset.npy is already loaded into dungeons_data\n",
        "# dungeons_data.shape should be (253, 8, 8)\n",
        "\n",
        "# Function to augment dataset\n",
        "def augment_dataset(dataset):\n",
        "    augmented_images = []\n",
        "\n",
        "    for image in dataset:\n",
        "        augmented_images.append(image)  # Original image\n",
        "\n",
        "        # Rotate by 90 degrees\n",
        "        rotated_90 = rotate(image, 90, reshape=False)\n",
        "        augmented_images.append(rotated_90)\n",
        "\n",
        "        # Rotate by 180 degrees\n",
        "        rotated_180 = rotate(image, 180, reshape=False)\n",
        "        augmented_images.append(rotated_180)\n",
        "\n",
        "        # Rotate by 270 degrees\n",
        "        rotated_270 = rotate(image, 270, reshape=False)\n",
        "        augmented_images.append(rotated_270)\n",
        "\n",
        "        # Flip horizontally\n",
        "        flipped_horizontal = np.fliplr(image)\n",
        "        augmented_images.append(flipped_horizontal)\n",
        "\n",
        "        # Flip vertically\n",
        "        flipped_vertical = np.flipud(image)\n",
        "        augmented_images.append(flipped_vertical)\n",
        "\n",
        "    return np.array(augmented_images)\n",
        "\n",
        "# Augment the dataset\n",
        "augmented_data = augment_dataset(dungeons_data)\n",
        "print(f\"Original dataset shape: {dungeons_data.shape}\")\n",
        "print(f\"Augmented dataset shape: {augmented_data.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQoKNuYI1TWS",
        "outputId": "15432eb3-96d6-4d3f-f17f-607da8814376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape: (253, 8, 8)\n",
            "Augmented dataset shape: (1518, 8, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualise a subset"
      ],
      "metadata": {
        "id": "gn1MxFVi1jE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming dungeons_dataset.npy is already loaded into dungeons_data\n",
        "# dungeons_data.shape should be (253, 8, 8)\n",
        "\n",
        "# Function to plot a subset of the dataset with correct colors\n",
        "def plot_dataset_subset(dataset, num_images_to_plot=10):\n",
        "    num_cols = 5  # Number of columns in the plot grid\n",
        "    num_rows = (num_images_to_plot + num_cols - 1) // num_cols  # Calculate number of rows based on num_images_to_plot\n",
        "\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 12))\n",
        "\n",
        "    for i in range(num_images_to_plot):\n",
        "        ax = axes.flat[i]\n",
        "        ax.imshow(dataset[i], cmap='gray', vmin=0, vmax=1)  # Use 'gray' colormap with vmin=0 and vmax=1\n",
        "        ax.set_title(f\"Image {i+1}\")\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Hide any unused subplot axes\n",
        "    for j in range(num_images_to_plot, num_rows*num_cols):\n",
        "        axes.flat[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize a subset of the dataset\n",
        "num_images_to_plot = 10  # Adjust the number of images to plot\n",
        "plot_dataset_subset(augmented_data, num_images_to_plot)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "IXwoCjpj1kr7",
        "outputId": "a8f78731-87c1-4bfc-8304-46c354714a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Normalize & Formatting"
      ],
      "metadata": {
        "id": "vthN3rRe2BAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming dungeons_dataset.npy is already loaded into dungeons_data\n",
        "# dungeons_data.shape should be (253, 8, 8)\n",
        "\n",
        "# Step 1: Normalize the dataset\n",
        "normalized_data = augmented_data.astype(np.float32)  # Convert to float32 for compatibility\n",
        "normalized_data /= 1.0  # Scale to range [0, 1]\n",
        "\n",
        "# Step 2: Reshape the dataset\n",
        "num_samples = augmented_data.shape[0]\n",
        "flattened_size = np.prod(augmented_data.shape[1:])  # 8*8 = 64\n",
        "reshaped_data = normalized_data.reshape(num_samples, flattened_size)\n",
        "\n",
        "# Example: Print shapes before and after normalization and reshaping\n",
        "print(f\"Original dataset shape: {augmented_data.shape}\")\n",
        "print(f\"Normalized dataset shape: {normalized_data.shape}\")\n",
        "print(f\"Reshaped dataset shape: {reshaped_data.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA85oMo72DLi",
        "outputId": "888f45b3-2fba-4099-fd18-f34d0f0ce578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape: (1518, 8, 8)\n",
            "Normalized dataset shape: (1518, 8, 8)\n",
            "Reshaped dataset shape: (1518, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split the dataset"
      ],
      "metadata": {
        "id": "9I0Y0dBw2RPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming reshaped_data is already prepared from the previous steps\n",
        "# reshaped_data.shape should be (253, 64)\n",
        "\n",
        "# Splitting the dataset into train and test sets\n",
        "train_data, test_data = train_test_split(reshaped_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Example: Print shapes of train and test sets\n",
        "print(f\"Train dataset shape: {train_data.shape}\")\n",
        "print(f\"Test dataset shape: {test_data.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igS01SIB2Sdi",
        "outputId": "8f9cca8c-7129-48bd-e435-7d0e59a95a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset shape: (1214, 64)\n",
            "Test dataset shape: (304, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VAE Model"
      ],
      "metadata": {
        "id": "c7AOI8kb3Xby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build and train"
      ],
      "metadata": {
        "id": "nrKRybp03ZMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw8li_9m3atT",
        "outputId": "6cd80fda-6014-4cad-9157-a50f1130c7b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the sampling function\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.random.normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# Define the VAE model\n",
        "latent_dim = 8  # Dimensionality of the latent space\n",
        "\n",
        "# Encoder network\n",
        "encoder_inputs = tf.keras.Input(shape=(64,))\n",
        "x = layers.Dense(32, activation='relu')(encoder_inputs)\n",
        "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
        "z = layers.Lambda(sampling, name='z')([z_mean, z_log_var])\n",
        "\n",
        "encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()\n",
        "\n",
        "# Decoder network\n",
        "latent_inputs = tf.keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(32, activation='relu')(latent_inputs)\n",
        "outputs = layers.Dense(64, activation='sigmoid')(x)  # Sigmoid activation for binary outputs\n",
        "\n",
        "decoder = models.Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()\n",
        "\n",
        "# VAE model\n",
        "outputs = decoder(encoder(encoder_inputs)[2])\n",
        "vae = models.Model(encoder_inputs, outputs, name='vae')\n",
        "\n",
        "# Loss function\n",
        "reconstruction_loss = tf.keras.losses.binary_crossentropy(encoder_inputs, outputs)\n",
        "reconstruction_loss *= 64  # Since binary cross-entropy gives the mean over all dimensions\n",
        "\n",
        "kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "kl_loss = tf.reduce_sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "\n",
        "vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "\n",
        "# Compile the VAE model\n",
        "vae.compile(optimizer='adam')\n",
        "\n",
        "# Train the VAE model\n",
        "history = vae.fit(train_data, epochs=1000, batch_size=32, validation_data=(test_data, None))\n",
        "\n",
        "# Function to generate an image from random latent vector\n",
        "def generate_image(decoder, latent_dim=8):\n",
        "    # Sample a random point from the latent space\n",
        "    latent_sample = np.random.normal(size=(1, latent_dim)).astype('float32')\n",
        "\n",
        "    # Decode the latent vector to get the generated image\n",
        "    generated_image = decoder.predict(latent_sample)\n",
        "\n",
        "    return generated_image.reshape(8, 8)  # Reshape to original image dimensions\n",
        "\n",
        "# Generate and visualize an image\n",
        "generated_image = generate_image(decoder)\n",
        "\n",
        "# Threshold to convert grayscale to binary\n",
        "binary_image = (generated_image > 0.5).astype(int)\n",
        "\n",
        "# Plot the generated binary image\n",
        "plt.imshow(binary_image, cmap='binary')\n",
        "plt.title('Generated Dungeon Layout')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3RDe7X9b3dbi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6dd4ec96-85d7-4db7-e226-17a43fd1a80a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)        [(None, 64)]                 0         []                            \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 32)                   2080      ['input_7[0][0]']             \n",
            "                                                                                                  \n",
            " z_mean (Dense)              (None, 8)                    264       ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " z_log_var (Dense)           (None, 8)                    264       ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " z (Lambda)                  (None, 8)                    0         ['z_mean[0][0]',              \n",
            "                                                                     'z_log_var[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2608 (10.19 KB)\n",
            "Trainable params: 2608 (10.19 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 8)]               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 32)                288       \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 64)                2112      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2400 (9.38 KB)\n",
            "Trainable params: 2400 (9.38 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "38/38 [==============================] - 2s 9ms/step - loss: 45.2051 - val_loss: 43.1397\n",
            "Epoch 2/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 41.3726 - val_loss: 40.4503\n",
            "Epoch 3/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 39.1018 - val_loss: 38.7136\n",
            "Epoch 4/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 37.8422 - val_loss: 37.6577\n",
            "Epoch 5/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 37.2549 - val_loss: 37.1079\n",
            "Epoch 6/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 36.8447 - val_loss: 36.9534\n",
            "Epoch 7/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 36.6233 - val_loss: 36.6704\n",
            "Epoch 8/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 36.4479 - val_loss: 36.3688\n",
            "Epoch 9/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 36.2556 - val_loss: 36.2808\n",
            "Epoch 10/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 36.1361 - val_loss: 36.2771\n",
            "Epoch 11/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 36.0441 - val_loss: 36.1700\n",
            "Epoch 12/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 36.0493 - val_loss: 36.3254\n",
            "Epoch 13/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 35.8914 - val_loss: 36.0655\n",
            "Epoch 14/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 35.7808 - val_loss: 35.8689\n",
            "Epoch 15/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 35.7857 - val_loss: 35.9514\n",
            "Epoch 16/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 35.7523 - val_loss: 35.7127\n",
            "Epoch 17/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 35.5759 - val_loss: 35.8278\n",
            "Epoch 18/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 35.5492 - val_loss: 35.4129\n",
            "Epoch 19/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 35.4313 - val_loss: 35.6086\n",
            "Epoch 20/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 35.3091 - val_loss: 35.3708\n",
            "Epoch 21/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 35.2006 - val_loss: 35.4332\n",
            "Epoch 22/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 35.1025 - val_loss: 35.2571\n",
            "Epoch 23/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 35.0083 - val_loss: 35.0447\n",
            "Epoch 24/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 34.9845 - val_loss: 35.1750\n",
            "Epoch 25/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 34.9096 - val_loss: 34.9901\n",
            "Epoch 26/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 34.6954 - val_loss: 34.9793\n",
            "Epoch 27/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 34.5391 - val_loss: 34.8669\n",
            "Epoch 28/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 34.5933 - val_loss: 34.8006\n",
            "Epoch 29/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 34.4447 - val_loss: 34.5466\n",
            "Epoch 30/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 34.3434 - val_loss: 34.5527\n",
            "Epoch 31/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 34.4229 - val_loss: 34.4153\n",
            "Epoch 32/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 34.3150 - val_loss: 34.2843\n",
            "Epoch 33/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 34.2293 - val_loss: 34.5151\n",
            "Epoch 34/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 34.2565 - val_loss: 34.5211\n",
            "Epoch 35/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 34.1857 - val_loss: 34.2740\n",
            "Epoch 36/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.9496 - val_loss: 34.2928\n",
            "Epoch 37/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 34.0835 - val_loss: 34.3271\n",
            "Epoch 38/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 34.0990 - val_loss: 34.0570\n",
            "Epoch 39/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.9481 - val_loss: 34.1635\n",
            "Epoch 40/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.8206 - val_loss: 34.4352\n",
            "Epoch 41/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.8405 - val_loss: 34.0504\n",
            "Epoch 42/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.8732 - val_loss: 34.1847\n",
            "Epoch 43/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.7115 - val_loss: 33.9388\n",
            "Epoch 44/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.7141 - val_loss: 33.9360\n",
            "Epoch 45/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.5066 - val_loss: 33.9103\n",
            "Epoch 46/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.5991 - val_loss: 33.9258\n",
            "Epoch 47/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.4666 - val_loss: 33.7296\n",
            "Epoch 48/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 33.3961 - val_loss: 33.6879\n",
            "Epoch 49/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.3623 - val_loss: 33.7962\n",
            "Epoch 50/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.3370 - val_loss: 33.7177\n",
            "Epoch 51/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.3560 - val_loss: 33.8012\n",
            "Epoch 52/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.2649 - val_loss: 33.6374\n",
            "Epoch 53/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.1346 - val_loss: 33.5747\n",
            "Epoch 54/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 33.1455 - val_loss: 33.6532\n",
            "Epoch 55/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 33.0377 - val_loss: 33.4607\n",
            "Epoch 56/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 32.8936 - val_loss: 33.6232\n",
            "Epoch 57/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 33.0451 - val_loss: 33.5739\n",
            "Epoch 58/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 32.9482 - val_loss: 33.4400\n",
            "Epoch 59/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 32.9123 - val_loss: 33.3970\n",
            "Epoch 60/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 32.9352 - val_loss: 33.3393\n",
            "Epoch 61/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 32.7273 - val_loss: 33.1693\n",
            "Epoch 62/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 32.7670 - val_loss: 33.3232\n",
            "Epoch 63/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 32.6924 - val_loss: 33.2291\n",
            "Epoch 64/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 32.6844 - val_loss: 33.2825\n",
            "Epoch 65/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 32.5216 - val_loss: 33.2084\n",
            "Epoch 66/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 32.5945 - val_loss: 33.1603\n",
            "Epoch 67/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 32.4235 - val_loss: 33.0017\n",
            "Epoch 68/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 32.4090 - val_loss: 33.0760\n",
            "Epoch 69/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 32.4349 - val_loss: 33.0480\n",
            "Epoch 70/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 32.3362 - val_loss: 32.9253\n",
            "Epoch 71/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 32.2119 - val_loss: 33.0742\n",
            "Epoch 72/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 32.0736 - val_loss: 33.0768\n",
            "Epoch 73/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 31.9889 - val_loss: 32.9436\n",
            "Epoch 74/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 32.0209 - val_loss: 32.9345\n",
            "Epoch 75/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 31.9029 - val_loss: 32.8745\n",
            "Epoch 76/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 31.8648 - val_loss: 32.8882\n",
            "Epoch 77/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 31.7214 - val_loss: 32.9347\n",
            "Epoch 78/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 31.6053 - val_loss: 33.1429\n",
            "Epoch 79/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 31.4628 - val_loss: 32.9706\n",
            "Epoch 80/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 31.4473 - val_loss: 33.1147\n",
            "Epoch 81/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 31.2501 - val_loss: 32.6461\n",
            "Epoch 82/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 31.0014 - val_loss: 32.9911\n",
            "Epoch 83/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 30.7529 - val_loss: 32.9804\n",
            "Epoch 84/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 30.7186 - val_loss: 32.9030\n",
            "Epoch 85/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 30.5886 - val_loss: 32.9065\n",
            "Epoch 86/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 30.1639 - val_loss: 33.2174\n",
            "Epoch 87/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 29.8475 - val_loss: 32.9494\n",
            "Epoch 88/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 29.6696 - val_loss: 32.9554\n",
            "Epoch 89/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 29.2180 - val_loss: 32.9601\n",
            "Epoch 90/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 29.2892 - val_loss: 33.1426\n",
            "Epoch 91/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 28.6683 - val_loss: 33.3747\n",
            "Epoch 92/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 27.8575 - val_loss: 33.0004\n",
            "Epoch 93/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 27.4019 - val_loss: 33.1327\n",
            "Epoch 94/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 26.9289 - val_loss: 33.3337\n",
            "Epoch 95/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 26.0752 - val_loss: 33.3813\n",
            "Epoch 96/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 25.2449 - val_loss: 33.5275\n",
            "Epoch 97/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 24.9460 - val_loss: 33.5089\n",
            "Epoch 98/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 24.4514 - val_loss: 33.4992\n",
            "Epoch 99/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 22.7288 - val_loss: 33.4908\n",
            "Epoch 100/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 21.6250 - val_loss: 33.5212\n",
            "Epoch 101/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 20.8607 - val_loss: 33.4342\n",
            "Epoch 102/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 19.2733 - val_loss: 33.4910\n",
            "Epoch 103/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 17.8798 - val_loss: 33.6032\n",
            "Epoch 104/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 16.3815 - val_loss: 33.6354\n",
            "Epoch 105/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 14.2644 - val_loss: 33.6718\n",
            "Epoch 106/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 12.4195 - val_loss: 33.7467\n",
            "Epoch 107/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 10.3862 - val_loss: 33.4556\n",
            "Epoch 108/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 7.9015 - val_loss: 33.3728\n",
            "Epoch 109/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 5.5365 - val_loss: 33.8427\n",
            "Epoch 110/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 2.9248 - val_loss: 33.3684\n",
            "Epoch 111/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -0.1894 - val_loss: 33.0426\n",
            "Epoch 112/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -2.7861 - val_loss: 33.2572\n",
            "Epoch 113/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -7.0440 - val_loss: 32.2947\n",
            "Epoch 114/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -7.9201 - val_loss: 32.7321\n",
            "Epoch 115/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -13.7086 - val_loss: 31.9749\n",
            "Epoch 116/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -16.9614 - val_loss: 31.3862\n",
            "Epoch 117/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -22.0439 - val_loss: 31.0890\n",
            "Epoch 118/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -27.0381 - val_loss: 30.4174\n",
            "Epoch 119/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -32.2660 - val_loss: 29.1193\n",
            "Epoch 120/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -35.9296 - val_loss: 29.1000\n",
            "Epoch 121/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -43.0574 - val_loss: 28.4839\n",
            "Epoch 122/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -47.2524 - val_loss: 27.5049\n",
            "Epoch 123/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -54.3511 - val_loss: 25.7758\n",
            "Epoch 124/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -62.2361 - val_loss: 25.6508\n",
            "Epoch 125/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -55.2580 - val_loss: 24.3543\n",
            "Epoch 126/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -75.2009 - val_loss: 22.6315\n",
            "Epoch 127/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -81.5443 - val_loss: 20.4255\n",
            "Epoch 128/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -92.2993 - val_loss: 18.2789\n",
            "Epoch 129/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -99.1016 - val_loss: 18.4188\n",
            "Epoch 130/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -108.6854 - val_loss: 14.2563\n",
            "Epoch 131/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -119.3308 - val_loss: 14.5112\n",
            "Epoch 132/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -127.5547 - val_loss: 11.9371\n",
            "Epoch 133/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -139.4501 - val_loss: 10.4227\n",
            "Epoch 134/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -146.5545 - val_loss: 7.7777\n",
            "Epoch 135/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -165.2957 - val_loss: 4.7252\n",
            "Epoch 136/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -175.2443 - val_loss: 1.5451\n",
            "Epoch 137/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -160.0887 - val_loss: -0.5637\n",
            "Epoch 138/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -196.8222 - val_loss: -5.1721\n",
            "Epoch 139/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -210.0672 - val_loss: -8.0979\n",
            "Epoch 140/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -227.1628 - val_loss: -12.3693\n",
            "Epoch 141/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -236.1093 - val_loss: -14.7534\n",
            "Epoch 142/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -256.0588 - val_loss: -17.3126\n",
            "Epoch 143/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -272.8495 - val_loss: -20.0541\n",
            "Epoch 144/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -291.5701 - val_loss: -27.5706\n",
            "Epoch 145/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -214.5580 - val_loss: -31.4927\n",
            "Epoch 146/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -314.4141 - val_loss: -32.1296\n",
            "Epoch 147/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -333.7400 - val_loss: -37.8912\n",
            "Epoch 148/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -353.9740 - val_loss: -40.3409\n",
            "Epoch 149/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -370.0345 - val_loss: -43.0063\n",
            "Epoch 150/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -392.5706 - val_loss: -48.7884\n",
            "Epoch 151/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -410.7303 - val_loss: -53.6590\n",
            "Epoch 152/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -430.5504 - val_loss: -58.0690\n",
            "Epoch 153/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -456.0681 - val_loss: -63.8675\n",
            "Epoch 154/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -480.9082 - val_loss: -67.8887\n",
            "Epoch 155/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -498.3460 - val_loss: -72.9168\n",
            "Epoch 156/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -524.3563 - val_loss: -80.8731\n",
            "Epoch 157/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -548.4484 - val_loss: -85.3871\n",
            "Epoch 158/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -585.5378 - val_loss: -87.5786\n",
            "Epoch 159/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -588.7186 - val_loss: -99.0955\n",
            "Epoch 160/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -624.6219 - val_loss: -102.8777\n",
            "Epoch 161/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -654.3141 - val_loss: -109.3610\n",
            "Epoch 162/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -687.5474 - val_loss: -118.6585\n",
            "Epoch 163/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -714.2459 - val_loss: -123.4238\n",
            "Epoch 164/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -615.8572 - val_loss: -124.9602\n",
            "Epoch 165/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -764.1083 - val_loss: -134.9239\n",
            "Epoch 166/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -788.1998 - val_loss: -143.7394\n",
            "Epoch 167/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -828.2498 - val_loss: -147.2583\n",
            "Epoch 168/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -856.3600 - val_loss: -157.7938\n",
            "Epoch 169/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -896.8519 - val_loss: -166.3476\n",
            "Epoch 170/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -930.4082 - val_loss: -170.3185\n",
            "Epoch 171/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -967.4935 - val_loss: -180.0650\n",
            "Epoch 172/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1005.9703 - val_loss: -189.8136\n",
            "Epoch 173/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1028.9973 - val_loss: -199.3009\n",
            "Epoch 174/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1074.5410 - val_loss: -206.8930\n",
            "Epoch 175/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1119.8524 - val_loss: -214.2525\n",
            "Epoch 176/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1141.7178 - val_loss: -223.9516\n",
            "Epoch 177/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1195.3721 - val_loss: -225.1732\n",
            "Epoch 178/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1237.1858 - val_loss: -238.5070\n",
            "Epoch 179/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1277.2137 - val_loss: -254.5235\n",
            "Epoch 180/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1359.5327 - val_loss: -257.3958\n",
            "Epoch 181/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1371.4365 - val_loss: -278.1324\n",
            "Epoch 182/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1451.3046 - val_loss: -278.3901\n",
            "Epoch 183/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 1767.7590 - val_loss: -290.2608\n",
            "Epoch 184/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1543.9437 - val_loss: -305.1670\n",
            "Epoch 185/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1527.6937 - val_loss: -310.5598\n",
            "Epoch 186/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1586.3533 - val_loss: -319.1143\n",
            "Epoch 187/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1632.6453 - val_loss: -337.9433\n",
            "Epoch 188/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1694.8839 - val_loss: -351.4607\n",
            "Epoch 189/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1737.4807 - val_loss: -361.1705\n",
            "Epoch 190/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1784.6620 - val_loss: -372.0859\n",
            "Epoch 191/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1832.0283 - val_loss: -391.4462\n",
            "Epoch 192/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1898.8812 - val_loss: -389.4184\n",
            "Epoch 193/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1943.5574 - val_loss: -409.1083\n",
            "Epoch 194/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 15647.7021 - val_loss: -408.3738\n",
            "Epoch 195/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1980.3563 - val_loss: -422.9199\n",
            "Epoch 196/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -2084.6045 - val_loss: -448.5867\n",
            "Epoch 197/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -2150.2805 - val_loss: -457.7200\n",
            "Epoch 198/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -2213.1204 - val_loss: -464.7751\n",
            "Epoch 199/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -2259.4880 - val_loss: -478.1652\n",
            "Epoch 200/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -2341.7158 - val_loss: -495.2470\n",
            "Epoch 201/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -2374.3638 - val_loss: -500.8433\n",
            "Epoch 202/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -2490.0312 - val_loss: -516.9839\n",
            "Epoch 203/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -2488.6921 - val_loss: -533.3890\n",
            "Epoch 204/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -2587.8813 - val_loss: -554.4842\n",
            "Epoch 205/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -2560.2229 - val_loss: -565.6381\n",
            "Epoch 206/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -2675.1135 - val_loss: -586.1791\n",
            "Epoch 207/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -2775.4619 - val_loss: -584.4949\n",
            "Epoch 208/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -2831.1357 - val_loss: -612.6862\n",
            "Epoch 209/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -2893.1943 - val_loss: -626.8297\n",
            "Epoch 210/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -2971.6946 - val_loss: -639.6979\n",
            "Epoch 211/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -3036.1106 - val_loss: -656.4913\n",
            "Epoch 212/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -3156.3679 - val_loss: -675.6609\n",
            "Epoch 213/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -3201.6023 - val_loss: -691.3255\n",
            "Epoch 214/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -3221.4929 - val_loss: -721.9330\n",
            "Epoch 215/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -3332.3831 - val_loss: -726.8159\n",
            "Epoch 216/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -3399.9424 - val_loss: -744.0787\n",
            "Epoch 217/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -3498.2427 - val_loss: -755.3563\n",
            "Epoch 218/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -3605.6143 - val_loss: -786.4805\n",
            "Epoch 219/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1556.6635 - val_loss: -784.3757\n",
            "Epoch 220/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -3737.9539 - val_loss: -815.3569\n",
            "Epoch 221/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -3844.5645 - val_loss: -837.7399\n",
            "Epoch 222/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -3875.0703 - val_loss: -854.8235\n",
            "Epoch 223/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -4015.4785 - val_loss: -877.1948\n",
            "Epoch 224/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -3611.7644 - val_loss: -879.4321\n",
            "Epoch 225/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -4093.1982 - val_loss: -897.8859\n",
            "Epoch 226/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -4290.4756 - val_loss: -917.7240\n",
            "Epoch 227/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -4279.3662 - val_loss: -951.9382\n",
            "Epoch 228/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -4390.7437 - val_loss: -965.4252\n",
            "Epoch 229/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -4460.7095 - val_loss: -980.3445\n",
            "Epoch 230/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -4588.0239 - val_loss: -1003.2346\n",
            "Epoch 231/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -4610.3960 - val_loss: -1028.4207\n",
            "Epoch 232/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -4758.9038 - val_loss: -1052.4949\n",
            "Epoch 233/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -4812.1030 - val_loss: -1069.3792\n",
            "Epoch 234/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -5034.4043 - val_loss: -1088.1462\n",
            "Epoch 235/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -4972.2534 - val_loss: -1124.9290\n",
            "Epoch 236/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -5138.0205 - val_loss: -1135.4542\n",
            "Epoch 237/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -5214.7539 - val_loss: -1160.3259\n",
            "Epoch 238/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -5372.7456 - val_loss: -1182.7175\n",
            "Epoch 239/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -5444.8672 - val_loss: -1216.0583\n",
            "Epoch 240/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -5597.0469 - val_loss: -1239.9513\n",
            "Epoch 241/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -5591.7227 - val_loss: -1262.8535\n",
            "Epoch 242/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -5721.3501 - val_loss: -1302.8583\n",
            "Epoch 243/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -5787.4727 - val_loss: -1326.1930\n",
            "Epoch 244/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -5893.2607 - val_loss: -1368.1630\n",
            "Epoch 245/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -6041.2886 - val_loss: -1389.8813\n",
            "Epoch 246/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -6102.8867 - val_loss: -1408.5831\n",
            "Epoch 247/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -6254.8477 - val_loss: -1460.2030\n",
            "Epoch 248/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -6391.0552 - val_loss: -1486.5424\n",
            "Epoch 249/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -6532.3062 - val_loss: -1491.6091\n",
            "Epoch 250/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -6578.1396 - val_loss: -1535.7410\n",
            "Epoch 251/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -6741.8213 - val_loss: -1564.0688\n",
            "Epoch 252/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -6838.7642 - val_loss: -1601.2416\n",
            "Epoch 253/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -6972.4399 - val_loss: -1613.9595\n",
            "Epoch 254/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -7081.7041 - val_loss: -1649.2434\n",
            "Epoch 255/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -7220.3848 - val_loss: -1705.3704\n",
            "Epoch 256/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -7420.0898 - val_loss: -1736.1543\n",
            "Epoch 257/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -7507.2666 - val_loss: -1764.5990\n",
            "Epoch 258/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -7640.2041 - val_loss: -1805.4304\n",
            "Epoch 259/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -7746.5820 - val_loss: -1841.1719\n",
            "Epoch 260/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -7934.4351 - val_loss: -1886.6023\n",
            "Epoch 261/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -8067.3438 - val_loss: -1933.3621\n",
            "Epoch 262/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -8203.9229 - val_loss: -1965.9460\n",
            "Epoch 263/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -8314.6846 - val_loss: -2022.2463\n",
            "Epoch 264/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -8486.3398 - val_loss: -2024.9929\n",
            "Epoch 265/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -8745.4775 - val_loss: -2060.2061\n",
            "Epoch 266/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -8788.7568 - val_loss: -2132.1174\n",
            "Epoch 267/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -8985.5449 - val_loss: -2151.5964\n",
            "Epoch 268/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -9063.9473 - val_loss: -2215.1499\n",
            "Epoch 269/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -9273.8311 - val_loss: -2285.7009\n",
            "Epoch 270/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -9472.3535 - val_loss: -2325.6880\n",
            "Epoch 271/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -9573.0586 - val_loss: -2357.0676\n",
            "Epoch 272/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -9973.3604 - val_loss: -2397.8040\n",
            "Epoch 273/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -10160.2871 - val_loss: -2456.5708\n",
            "Epoch 274/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -10100.4512 - val_loss: -2517.0410\n",
            "Epoch 275/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -10264.7920 - val_loss: -2571.0122\n",
            "Epoch 276/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -10467.2705 - val_loss: -2617.5901\n",
            "Epoch 277/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -10614.8330 - val_loss: -2675.4692\n",
            "Epoch 278/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -10829.0010 - val_loss: -2714.9976\n",
            "Epoch 279/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -11486.9873 - val_loss: -2754.4922\n",
            "Epoch 280/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -11085.2852 - val_loss: -2830.6890\n",
            "Epoch 281/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -11334.5078 - val_loss: -2888.5891\n",
            "Epoch 282/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -11532.3252 - val_loss: -2944.5159\n",
            "Epoch 283/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -11741.5605 - val_loss: -2991.6606\n",
            "Epoch 284/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -11917.0332 - val_loss: -3109.2676\n",
            "Epoch 285/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -12158.3877 - val_loss: -3149.2432\n",
            "Epoch 286/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -12362.6035 - val_loss: -3214.3604\n",
            "Epoch 287/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -12473.7568 - val_loss: -3270.0015\n",
            "Epoch 288/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -12748.6689 - val_loss: -3345.2090\n",
            "Epoch 289/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -12776.8213 - val_loss: -3407.2520\n",
            "Epoch 290/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -13073.4492 - val_loss: -3481.5195\n",
            "Epoch 291/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -13346.3320 - val_loss: -3545.6028\n",
            "Epoch 292/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -13536.7783 - val_loss: -3575.4575\n",
            "Epoch 293/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -13455.9980 - val_loss: -3676.1301\n",
            "Epoch 294/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -13984.5557 - val_loss: -3735.6262\n",
            "Epoch 295/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -14211.4922 - val_loss: -3786.4915\n",
            "Epoch 296/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -14477.6514 - val_loss: -3843.3823\n",
            "Epoch 297/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -14650.8359 - val_loss: -3911.3289\n",
            "Epoch 298/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -14908.5479 - val_loss: -4012.0364\n",
            "Epoch 299/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -15119.4180 - val_loss: -4079.1948\n",
            "Epoch 300/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -15375.4131 - val_loss: -4170.7168\n",
            "Epoch 301/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -15581.4814 - val_loss: -4245.9409\n",
            "Epoch 302/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -15827.4326 - val_loss: -4312.3452\n",
            "Epoch 303/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -16109.9717 - val_loss: -4417.8281\n",
            "Epoch 304/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -16343.2334 - val_loss: -4498.6318\n",
            "Epoch 305/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -16602.3281 - val_loss: -4576.3779\n",
            "Epoch 306/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -16831.5156 - val_loss: -4681.1060\n",
            "Epoch 307/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -17161.9238 - val_loss: -4707.1602\n",
            "Epoch 308/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -17420.6719 - val_loss: -4799.5767\n",
            "Epoch 309/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -17708.4141 - val_loss: -4873.5439\n",
            "Epoch 310/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -17895.5234 - val_loss: -4980.7451\n",
            "Epoch 311/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -18233.7266 - val_loss: -5065.6084\n",
            "Epoch 312/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -18503.0215 - val_loss: -5163.3774\n",
            "Epoch 313/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -18816.1406 - val_loss: -5284.3193\n",
            "Epoch 314/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -19071.9277 - val_loss: -5349.0322\n",
            "Epoch 315/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -19562.3848 - val_loss: -5467.5088\n",
            "Epoch 316/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -19646.9805 - val_loss: -5599.4131\n",
            "Epoch 317/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -19999.5352 - val_loss: -5737.4834\n",
            "Epoch 318/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -20311.5039 - val_loss: -5776.9785\n",
            "Epoch 319/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -20544.0215 - val_loss: -5831.7539\n",
            "Epoch 320/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -20884.8887 - val_loss: -5999.1455\n",
            "Epoch 321/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -21141.1387 - val_loss: -6122.4917\n",
            "Epoch 322/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -21542.4062 - val_loss: -6204.0293\n",
            "Epoch 323/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -21883.4570 - val_loss: -6280.4731\n",
            "Epoch 324/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -22111.9941 - val_loss: -6446.9194\n",
            "Epoch 325/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -22511.2441 - val_loss: -6558.7109\n",
            "Epoch 326/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -23155.2559 - val_loss: -6671.0557\n",
            "Epoch 327/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -23164.4434 - val_loss: -6752.0640\n",
            "Epoch 328/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -23507.2266 - val_loss: -6849.7407\n",
            "Epoch 329/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -23920.1855 - val_loss: -6977.7422\n",
            "Epoch 330/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -24318.1133 - val_loss: -7082.2354\n",
            "Epoch 331/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -24632.9824 - val_loss: -7255.8696\n",
            "Epoch 332/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -25062.7578 - val_loss: -7308.6133\n",
            "Epoch 333/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -25284.7949 - val_loss: -7502.9209\n",
            "Epoch 334/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -16905.1621 - val_loss: -7559.3047\n",
            "Epoch 335/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -25730.9043 - val_loss: -7649.2290\n",
            "Epoch 336/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -26245.2969 - val_loss: -7734.9165\n",
            "Epoch 337/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -26581.0996 - val_loss: -7873.4976\n",
            "Epoch 338/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -27013.2402 - val_loss: -8026.3359\n",
            "Epoch 339/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -27383.5820 - val_loss: -8077.9336\n",
            "Epoch 340/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -27961.4316 - val_loss: -8317.7002\n",
            "Epoch 341/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -28098.7012 - val_loss: -8318.3125\n",
            "Epoch 342/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -28499.4219 - val_loss: -8410.2822\n",
            "Epoch 343/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -28896.8379 - val_loss: -8615.1592\n",
            "Epoch 344/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -29272.2598 - val_loss: -8687.5742\n",
            "Epoch 345/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -29716.6055 - val_loss: -8827.5879\n",
            "Epoch 346/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -30236.1836 - val_loss: -8909.6504\n",
            "Epoch 347/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -30489.4082 - val_loss: -9130.5469\n",
            "Epoch 348/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -30769.3359 - val_loss: -9213.5713\n",
            "Epoch 349/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -31415.8223 - val_loss: -9306.2764\n",
            "Epoch 350/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -31773.6094 - val_loss: -9491.6270\n",
            "Epoch 351/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -32271.7305 - val_loss: -9586.5566\n",
            "Epoch 352/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -32404.7109 - val_loss: -9825.7344\n",
            "Epoch 353/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -33092.0430 - val_loss: -9859.8887\n",
            "Epoch 354/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -33263.5859 - val_loss: -9984.8047\n",
            "Epoch 355/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -33907.7188 - val_loss: -10115.3301\n",
            "Epoch 356/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -34759.7148 - val_loss: -10369.8740\n",
            "Epoch 357/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -34456.3438 - val_loss: -10512.9590\n",
            "Epoch 358/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -35201.9688 - val_loss: -10590.6602\n",
            "Epoch 359/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -35724.9883 - val_loss: -10741.8027\n",
            "Epoch 360/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -36122.7109 - val_loss: -10930.7900\n",
            "Epoch 361/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -36798.2852 - val_loss: -10998.2793\n",
            "Epoch 362/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -36935.3672 - val_loss: -11171.7285\n",
            "Epoch 363/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -38074.0000 - val_loss: -11348.6309\n",
            "Epoch 364/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -38648.2188 - val_loss: -11472.2324\n",
            "Epoch 365/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -38503.5391 - val_loss: -11576.9082\n",
            "Epoch 366/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -38958.3086 - val_loss: -11826.2764\n",
            "Epoch 367/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -39493.0312 - val_loss: -11966.6836\n",
            "Epoch 368/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -40492.6445 - val_loss: -12167.8662\n",
            "Epoch 369/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -41813.4727 - val_loss: -12213.9707\n",
            "Epoch 370/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -40970.9336 - val_loss: -12511.5654\n",
            "Epoch 371/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -41648.0312 - val_loss: -12569.1221\n",
            "Epoch 372/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -42753.3242 - val_loss: -12770.3281\n",
            "Epoch 373/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -42752.0352 - val_loss: -12788.0742\n",
            "Epoch 374/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -42611.0898 - val_loss: -13093.2441\n",
            "Epoch 375/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -43798.3555 - val_loss: -13277.3408\n",
            "Epoch 376/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -44164.3242 - val_loss: -13448.7324\n",
            "Epoch 377/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -44486.4062 - val_loss: -13680.1426\n",
            "Epoch 378/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -45433.3438 - val_loss: -13686.7441\n",
            "Epoch 379/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -45981.6172 - val_loss: -13894.3770\n",
            "Epoch 380/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -46427.4062 - val_loss: -14195.0420\n",
            "Epoch 381/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -47640.1367 - val_loss: -14320.1592\n",
            "Epoch 382/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -47614.8086 - val_loss: -14495.9404\n",
            "Epoch 383/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -48310.5391 - val_loss: -14745.9863\n",
            "Epoch 384/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -48910.3867 - val_loss: -14929.6113\n",
            "Epoch 385/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -49502.9219 - val_loss: -15091.0000\n",
            "Epoch 386/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -50126.5508 - val_loss: -15212.0283\n",
            "Epoch 387/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -50597.2070 - val_loss: -15491.2637\n",
            "Epoch 388/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -51321.6445 - val_loss: -15566.5098\n",
            "Epoch 389/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -52000.7422 - val_loss: -15891.7832\n",
            "Epoch 390/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -52593.6172 - val_loss: -16161.2988\n",
            "Epoch 391/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -53273.5781 - val_loss: -16162.0176\n",
            "Epoch 392/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -53808.6133 - val_loss: -16402.7285\n",
            "Epoch 393/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -54500.0938 - val_loss: -16695.2051\n",
            "Epoch 394/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -55102.7812 - val_loss: -16876.0371\n",
            "Epoch 395/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -55847.5977 - val_loss: -17005.9883\n",
            "Epoch 396/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -56512.1172 - val_loss: -17281.0703\n",
            "Epoch 397/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -57232.7227 - val_loss: -17487.2246\n",
            "Epoch 398/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -57836.8164 - val_loss: -17748.5664\n",
            "Epoch 399/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -59395.2422 - val_loss: -17980.1836\n",
            "Epoch 400/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -59863.1328 - val_loss: -18079.2988\n",
            "Epoch 401/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -59844.5078 - val_loss: -18473.7031\n",
            "Epoch 402/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -62699.2227 - val_loss: -18477.9766\n",
            "Epoch 403/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -61445.8086 - val_loss: -18845.7227\n",
            "Epoch 404/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -62085.0312 - val_loss: -19150.6523\n",
            "Epoch 405/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -62949.8086 - val_loss: -19291.4453\n",
            "Epoch 406/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -63699.6328 - val_loss: -19636.1543\n",
            "Epoch 407/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -64335.5977 - val_loss: -19714.0898\n",
            "Epoch 408/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -65056.0195 - val_loss: -19847.1309\n",
            "Epoch 409/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -65891.9062 - val_loss: -20227.4258\n",
            "Epoch 410/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -66643.1016 - val_loss: -20546.9844\n",
            "Epoch 411/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -67425.6250 - val_loss: -20767.7168\n",
            "Epoch 412/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -68202.2812 - val_loss: -20910.8535\n",
            "Epoch 413/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -69015.5312 - val_loss: -21233.0195\n",
            "Epoch 414/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -69896.6094 - val_loss: -21447.7773\n",
            "Epoch 415/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -70639.8516 - val_loss: -21623.8496\n",
            "Epoch 416/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -71352.5391 - val_loss: -21879.8242\n",
            "Epoch 417/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -72207.8125 - val_loss: -22309.7344\n",
            "Epoch 418/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -73112.1250 - val_loss: -22550.8184\n",
            "Epoch 419/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -73997.6094 - val_loss: -23042.1914\n",
            "Epoch 420/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 63047.6367 - val_loss: -23191.3262\n",
            "Epoch 421/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -74953.1016 - val_loss: -23384.9512\n",
            "Epoch 422/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -75750.8125 - val_loss: -23617.4023\n",
            "Epoch 423/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -76723.9844 - val_loss: -23940.4551\n",
            "Epoch 424/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -77412.6719 - val_loss: -24115.9902\n",
            "Epoch 425/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -78285.4141 - val_loss: -24503.4512\n",
            "Epoch 426/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -79166.8125 - val_loss: -24655.5410\n",
            "Epoch 427/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -79906.8047 - val_loss: -24929.2480\n",
            "Epoch 428/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -81520.9453 - val_loss: -25105.2598\n",
            "Epoch 429/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -82563.9844 - val_loss: -25511.7852\n",
            "Epoch 430/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -82198.4375 - val_loss: -25705.6094\n",
            "Epoch 431/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -83057.0000 - val_loss: -26096.5117\n",
            "Epoch 432/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -83844.0156 - val_loss: -26343.2246\n",
            "Epoch 433/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -84759.1562 - val_loss: -26680.5215\n",
            "Epoch 434/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -85652.0625 - val_loss: -26775.8320\n",
            "Epoch 435/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -86446.5234 - val_loss: -27220.1211\n",
            "Epoch 436/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -87416.4297 - val_loss: -27420.7871\n",
            "Epoch 437/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -88148.3750 - val_loss: -27761.2871\n",
            "Epoch 438/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -88789.2656 - val_loss: -28120.7246\n",
            "Epoch 439/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -89598.8203 - val_loss: -28439.8633\n",
            "Epoch 440/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -90684.5859 - val_loss: -28828.5410\n",
            "Epoch 441/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -91474.1484 - val_loss: -28913.2500\n",
            "Epoch 442/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -92289.8672 - val_loss: -29357.5840\n",
            "Epoch 443/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -93310.2891 - val_loss: -29646.2441\n",
            "Epoch 444/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -94175.6094 - val_loss: -29987.2246\n",
            "Epoch 445/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -95243.8516 - val_loss: -30115.4629\n",
            "Epoch 446/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -96087.6016 - val_loss: -30538.3184\n",
            "Epoch 447/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -96878.6719 - val_loss: -31016.0508\n",
            "Epoch 448/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -97693.4844 - val_loss: -31109.0684\n",
            "Epoch 449/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -98657.4844 - val_loss: -31494.9961\n",
            "Epoch 450/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -99598.5625 - val_loss: -31953.8340\n",
            "Epoch 451/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -100574.2500 - val_loss: -32092.9590\n",
            "Epoch 452/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -101517.8203 - val_loss: -32578.4492\n",
            "Epoch 453/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -102506.9297 - val_loss: -32742.5508\n",
            "Epoch 454/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -103550.0234 - val_loss: -33180.8984\n",
            "Epoch 455/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -104360.5547 - val_loss: -33535.7109\n",
            "Epoch 456/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -105527.5078 - val_loss: -34038.0859\n",
            "Epoch 457/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -106295.9375 - val_loss: -34279.7031\n",
            "Epoch 458/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -107477.0859 - val_loss: -34685.5273\n",
            "Epoch 459/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -113392.1953 - val_loss: -34822.3516\n",
            "Epoch 460/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -109231.8125 - val_loss: -35367.5195\n",
            "Epoch 461/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -110681.6719 - val_loss: -35656.7383\n",
            "Epoch 462/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -111467.2891 - val_loss: -36110.9219\n",
            "Epoch 463/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -112364.4609 - val_loss: -36500.0117\n",
            "Epoch 464/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -117352.8984 - val_loss: -36624.2109\n",
            "Epoch 465/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -114450.5391 - val_loss: -37169.9180\n",
            "Epoch 466/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -115477.8047 - val_loss: -37658.2500\n",
            "Epoch 467/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -118094.9766 - val_loss: -38009.7461\n",
            "Epoch 468/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -121746.9219 - val_loss: -38338.7422\n",
            "Epoch 469/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -118844.8281 - val_loss: -38516.8359\n",
            "Epoch 470/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -119926.0625 - val_loss: -38926.4258\n",
            "Epoch 471/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -121021.2891 - val_loss: -39549.2930\n",
            "Epoch 472/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -122086.8984 - val_loss: -39873.0586\n",
            "Epoch 473/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -123205.1016 - val_loss: -40369.1992\n",
            "Epoch 474/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -124338.3281 - val_loss: -40776.7266\n",
            "Epoch 475/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -125567.8516 - val_loss: -41157.9453\n",
            "Epoch 476/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -126558.6484 - val_loss: -41511.4766\n",
            "Epoch 477/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -127716.5391 - val_loss: -41952.8359\n",
            "Epoch 478/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -128806.9375 - val_loss: -42321.5273\n",
            "Epoch 479/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -129853.9766 - val_loss: -42642.7578\n",
            "Epoch 480/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -131113.4062 - val_loss: -43182.1289\n",
            "Epoch 481/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -133125.0469 - val_loss: -43503.2031\n",
            "Epoch 482/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 11983109120.0000 - val_loss: -42569.6641\n",
            "Epoch 483/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -132895.9688 - val_loss: -42337.8320\n",
            "Epoch 484/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -132520.6250 - val_loss: -43170.6094\n",
            "Epoch 485/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -139121.9531 - val_loss: -43360.1758\n",
            "Epoch 486/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -134864.5625 - val_loss: -44125.2266\n",
            "Epoch 487/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -136532.3750 - val_loss: -44508.2539\n",
            "Epoch 488/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -136321.6250 - val_loss: -44916.3711\n",
            "Epoch 489/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -138916.3438 - val_loss: -45278.7930\n",
            "Epoch 490/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -139915.5469 - val_loss: -45799.3203\n",
            "Epoch 491/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -141471.4844 - val_loss: -46454.9961\n",
            "Epoch 492/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -140104.2969 - val_loss: -46743.4961\n",
            "Epoch 493/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -143252.5312 - val_loss: -47083.8828\n",
            "Epoch 494/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -142909.3750 - val_loss: -47637.2266\n",
            "Epoch 495/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -149674.4531 - val_loss: -48019.7891\n",
            "Epoch 496/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -145280.2031 - val_loss: -48472.3828\n",
            "Epoch 497/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -146356.3906 - val_loss: -49029.8047\n",
            "Epoch 498/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -150225.4219 - val_loss: -49513.0352\n",
            "Epoch 499/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -152609.7969 - val_loss: -49726.7422\n",
            "Epoch 500/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -149425.2500 - val_loss: -50395.5312\n",
            "Epoch 501/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -150063.6250 - val_loss: -50929.5977\n",
            "Epoch 502/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -152670.7812 - val_loss: -51330.3438\n",
            "Epoch 503/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -152889.9688 - val_loss: -51517.1953\n",
            "Epoch 504/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -155244.1562 - val_loss: -52009.3711\n",
            "Epoch 505/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -154831.1094 - val_loss: -52411.2578\n",
            "Epoch 506/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 18994064.0000 - val_loss: -52059.4414\n",
            "Epoch 507/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -157183.1562 - val_loss: -52910.7891\n",
            "Epoch 508/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -159080.0156 - val_loss: -53280.4609\n",
            "Epoch 509/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -159434.2344 - val_loss: -53921.2422\n",
            "Epoch 510/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -161471.7969 - val_loss: -54589.2695\n",
            "Epoch 511/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -160744.5312 - val_loss: -55069.6172\n",
            "Epoch 512/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 238206.6562 - val_loss: -55570.7500\n",
            "Epoch 513/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -164457.8438 - val_loss: -56211.4297\n",
            "Epoch 514/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -164949.8125 - val_loss: -56518.5117\n",
            "Epoch 515/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -166952.0312 - val_loss: -56685.8320\n",
            "Epoch 516/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -167118.6562 - val_loss: -57353.9688\n",
            "Epoch 517/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -166024.4531 - val_loss: -57764.4258\n",
            "Epoch 518/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -167690.3125 - val_loss: -58352.0312\n",
            "Epoch 519/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -168472.2656 - val_loss: -58473.3555\n",
            "Epoch 520/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -171499.6094 - val_loss: -58863.0508\n",
            "Epoch 521/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -169270.7031 - val_loss: -59623.9766\n",
            "Epoch 522/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -173232.9688 - val_loss: -59723.1680\n",
            "Epoch 523/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -171555.5938 - val_loss: -60124.1758\n",
            "Epoch 524/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -173056.5625 - val_loss: -60792.2930\n",
            "Epoch 525/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -176524.2812 - val_loss: -61355.0391\n",
            "Epoch 526/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -176479.0938 - val_loss: -61603.0000\n",
            "Epoch 527/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -178687.5781 - val_loss: -61881.9570\n",
            "Epoch 528/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -177344.9531 - val_loss: -62342.2930\n",
            "Epoch 529/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -177905.0625 - val_loss: -62690.6484\n",
            "Epoch 530/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -181542.6719 - val_loss: -63266.7500\n",
            "Epoch 531/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -179109.9531 - val_loss: -63536.0703\n",
            "Epoch 532/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -184559.1562 - val_loss: -63999.3633\n",
            "Epoch 533/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -184092.6562 - val_loss: -64241.5000\n",
            "Epoch 534/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -186237.8906 - val_loss: -64816.3516\n",
            "Epoch 535/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -186594.0781 - val_loss: -65270.4258\n",
            "Epoch 536/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -186547.0469 - val_loss: -65713.3281\n",
            "Epoch 537/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -187387.7188 - val_loss: -66170.2188\n",
            "Epoch 538/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -190412.5000 - val_loss: -66692.2656\n",
            "Epoch 539/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -189885.7656 - val_loss: -66863.3906\n",
            "Epoch 540/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -191440.4531 - val_loss: -67420.4062\n",
            "Epoch 541/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -194857.7031 - val_loss: -67985.6172\n",
            "Epoch 542/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -194768.3750 - val_loss: -68549.9922\n",
            "Epoch 543/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -196517.9531 - val_loss: -68823.1094\n",
            "Epoch 544/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -197129.7969 - val_loss: -69080.1875\n",
            "Epoch 545/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -198035.3750 - val_loss: -69775.3672\n",
            "Epoch 546/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -198445.8125 - val_loss: -69975.9219\n",
            "Epoch 547/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -201366.5156 - val_loss: -70640.0156\n",
            "Epoch 548/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -205657.6094 - val_loss: -70874.3047\n",
            "Epoch 549/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -201017.0781 - val_loss: -71411.2969\n",
            "Epoch 550/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -202211.0625 - val_loss: -71862.6719\n",
            "Epoch 551/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -204535.1250 - val_loss: -72109.3672\n",
            "Epoch 552/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -207393.8594 - val_loss: -72526.6953\n",
            "Epoch 553/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -206944.4219 - val_loss: -73110.3516\n",
            "Epoch 554/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -208091.5156 - val_loss: -73775.7656\n",
            "Epoch 555/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -209561.1875 - val_loss: -73750.7578\n",
            "Epoch 556/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -209398.4531 - val_loss: -74299.9453\n",
            "Epoch 557/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -212676.7656 - val_loss: -74870.8984\n",
            "Epoch 558/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -214855.2188 - val_loss: -75359.2109\n",
            "Epoch 559/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -214077.3594 - val_loss: -75527.6172\n",
            "Epoch 560/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -215806.9531 - val_loss: -76412.8672\n",
            "Epoch 561/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -216600.5469 - val_loss: -76531.5156\n",
            "Epoch 562/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -217500.0625 - val_loss: -77174.5547\n",
            "Epoch 563/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -219514.3281 - val_loss: -77699.1094\n",
            "Epoch 564/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -221618.6094 - val_loss: -78139.8516\n",
            "Epoch 565/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -221361.0469 - val_loss: -78844.3984\n",
            "Epoch 566/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -223882.6875 - val_loss: -79453.4141\n",
            "Epoch 567/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -225710.9531 - val_loss: -79690.5312\n",
            "Epoch 568/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -225512.1562 - val_loss: -79896.2344\n",
            "Epoch 569/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -230662.5469 - val_loss: -80878.3906\n",
            "Epoch 570/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -229749.3438 - val_loss: -80757.1328\n",
            "Epoch 571/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -228908.0312 - val_loss: -81336.1484\n",
            "Epoch 572/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -231132.6875 - val_loss: -82364.6797\n",
            "Epoch 573/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -234315.6250 - val_loss: -82598.3203\n",
            "Epoch 574/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -237126.5469 - val_loss: -83168.0859\n",
            "Epoch 575/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -238027.5312 - val_loss: -83679.1719\n",
            "Epoch 576/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -236855.5000 - val_loss: -84270.4844\n",
            "Epoch 577/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -239461.7031 - val_loss: -84817.1328\n",
            "Epoch 578/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -242210.9688 - val_loss: -84860.1719\n",
            "Epoch 579/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -243684.7656 - val_loss: -85946.4219\n",
            "Epoch 580/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -249499.1094 - val_loss: -86115.6641\n",
            "Epoch 581/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -245810.1562 - val_loss: -87015.0156\n",
            "Epoch 582/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -245869.7344 - val_loss: -88247.9531\n",
            "Epoch 583/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -250104.7656 - val_loss: -87368.4219\n",
            "Epoch 584/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -251171.4531 - val_loss: -88548.7734\n",
            "Epoch 585/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -248855.3750 - val_loss: -87516.1953\n",
            "Epoch 586/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -253319.2969 - val_loss: -89093.1641\n",
            "Epoch 587/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -254011.6875 - val_loss: -90143.1953\n",
            "Epoch 588/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -258420.2031 - val_loss: -91025.7812\n",
            "Epoch 589/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -256136.1562 - val_loss: -90162.3906\n",
            "Epoch 590/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -258815.8750 - val_loss: -92428.4609\n",
            "Epoch 591/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -269795.9688 - val_loss: -91708.2656\n",
            "Epoch 592/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 630937.1875 - val_loss: -91595.0938\n",
            "Epoch 593/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 284014496.0000 - val_loss: -91480.8438\n",
            "Epoch 594/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -266062.6875 - val_loss: -98667.9844\n",
            "Epoch 595/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -264834.2812 - val_loss: -90567.7031\n",
            "Epoch 596/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -270250.4375 - val_loss: -94280.7422\n",
            "Epoch 597/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -271061.1250 - val_loss: -95600.6953\n",
            "Epoch 598/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -269442.5938 - val_loss: -90958.7031\n",
            "Epoch 599/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -269643.3125 - val_loss: -94855.3516\n",
            "Epoch 600/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -274898.1875 - val_loss: -96717.1484\n",
            "Epoch 601/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -279845.6562 - val_loss: -96007.5391\n",
            "Epoch 602/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -274399.9375 - val_loss: -96892.0547\n",
            "Epoch 603/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -277284.6562 - val_loss: -96200.8906\n",
            "Epoch 604/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -278058.5938 - val_loss: -98267.5938\n",
            "Epoch 605/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -281791.9375 - val_loss: -97634.0938\n",
            "Epoch 606/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -282559.5938 - val_loss: -97770.2031\n",
            "Epoch 607/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -283118.2500 - val_loss: -98882.0781\n",
            "Epoch 608/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -295559.6562 - val_loss: -98228.8359\n",
            "Epoch 609/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -286487.7500 - val_loss: -101090.1562\n",
            "Epoch 610/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -285546.7188 - val_loss: -100225.4219\n",
            "Epoch 611/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -288172.4375 - val_loss: -96588.5156\n",
            "Epoch 612/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -288709.9375 - val_loss: -99140.8281\n",
            "Epoch 613/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -292454.8125 - val_loss: -99468.9062\n",
            "Epoch 614/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -294965.5938 - val_loss: -103250.2656\n",
            "Epoch 615/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -295782.8125 - val_loss: -102109.7656\n",
            "Epoch 616/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -308181.0938 - val_loss: -104005.0625\n",
            "Epoch 617/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -296633.5938 - val_loss: -103908.7891\n",
            "Epoch 618/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -299883.6875 - val_loss: -103992.5312\n",
            "Epoch 619/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -299033.7188 - val_loss: -104708.5781\n",
            "Epoch 620/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -300945.2188 - val_loss: -105485.5469\n",
            "Epoch 621/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -305516.6875 - val_loss: -105757.6016\n",
            "Epoch 622/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -317139.0000 - val_loss: -106668.0781\n",
            "Epoch 623/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -305748.2188 - val_loss: -106443.7969\n",
            "Epoch 624/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -309519.3438 - val_loss: -107081.2266\n",
            "Epoch 625/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -309527.3750 - val_loss: -107601.5469\n",
            "Epoch 626/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -312847.7500 - val_loss: -108401.2500\n",
            "Epoch 627/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -314052.8750 - val_loss: -109408.5781\n",
            "Epoch 628/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -313102.2188 - val_loss: -108934.5391\n",
            "Epoch 629/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -313735.3438 - val_loss: -109657.5156\n",
            "Epoch 630/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -316858.4375 - val_loss: -110468.9531\n",
            "Epoch 631/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -319549.9062 - val_loss: -110384.3906\n",
            "Epoch 632/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -320444.1875 - val_loss: -110731.1016\n",
            "Epoch 633/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -321169.9688 - val_loss: -111592.4219\n",
            "Epoch 634/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -322983.3750 - val_loss: -112434.7109\n",
            "Epoch 635/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -328544.8750 - val_loss: -113090.1719\n",
            "Epoch 636/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -325467.9688 - val_loss: -113995.9844\n",
            "Epoch 637/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -330277.8750 - val_loss: -114025.3281\n",
            "Epoch 638/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -329861.5625 - val_loss: -115134.0234\n",
            "Epoch 639/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -331911.0625 - val_loss: -114787.7734\n",
            "Epoch 640/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -333502.3438 - val_loss: -114974.7344\n",
            "Epoch 641/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -336743.2500 - val_loss: -116188.9531\n",
            "Epoch 642/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -337483.0938 - val_loss: -116955.1484\n",
            "Epoch 643/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -340415.9062 - val_loss: -117273.4375\n",
            "Epoch 644/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -339740.5000 - val_loss: -117225.3359\n",
            "Epoch 645/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -344460.4375 - val_loss: -117960.5859\n",
            "Epoch 646/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -345461.4062 - val_loss: -119303.4141\n",
            "Epoch 647/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -348064.6562 - val_loss: -119441.2344\n",
            "Epoch 648/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -347625.2188 - val_loss: -120092.9531\n",
            "Epoch 649/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -347299.5938 - val_loss: -120395.8359\n",
            "Epoch 650/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -349720.5938 - val_loss: -122288.5234\n",
            "Epoch 651/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -352985.3750 - val_loss: -121782.3516\n",
            "Epoch 652/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -355389.3125 - val_loss: -123108.1875\n",
            "Epoch 653/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -356720.6562 - val_loss: -122723.2344\n",
            "Epoch 654/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -358089.2500 - val_loss: -123751.1641\n",
            "Epoch 655/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -359819.5000 - val_loss: -124052.5625\n",
            "Epoch 656/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -245029.9531 - val_loss: -124942.2109\n",
            "Epoch 657/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -363116.3438 - val_loss: -125632.2656\n",
            "Epoch 658/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -365569.3438 - val_loss: -125464.2969\n",
            "Epoch 659/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -367383.1875 - val_loss: -126591.4844\n",
            "Epoch 660/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -368529.0000 - val_loss: -127099.3125\n",
            "Epoch 661/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -368935.8750 - val_loss: -127718.3750\n",
            "Epoch 662/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -374342.9375 - val_loss: -128432.5391\n",
            "Epoch 663/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -375847.1250 - val_loss: -129271.9531\n",
            "Epoch 664/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -376586.5000 - val_loss: -129176.9375\n",
            "Epoch 665/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -377375.5938 - val_loss: -130304.9844\n",
            "Epoch 666/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -380385.9688 - val_loss: -130969.4609\n",
            "Epoch 667/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -383511.1875 - val_loss: -131561.8125\n",
            "Epoch 668/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -383828.2500 - val_loss: -132489.7812\n",
            "Epoch 669/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -385681.2500 - val_loss: -132508.2500\n",
            "Epoch 670/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -390052.5312 - val_loss: -133161.4531\n",
            "Epoch 671/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -389777.4688 - val_loss: -134149.5625\n",
            "Epoch 672/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -391543.0625 - val_loss: -134593.9062\n",
            "Epoch 673/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -392606.4062 - val_loss: -135674.6094\n",
            "Epoch 674/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -396049.0938 - val_loss: -135990.2500\n",
            "Epoch 675/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -397348.0938 - val_loss: -136431.8906\n",
            "Epoch 676/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -399456.0938 - val_loss: -137119.7188\n",
            "Epoch 677/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -403338.6875 - val_loss: -137602.5781\n",
            "Epoch 678/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -404561.7500 - val_loss: -138668.2031\n",
            "Epoch 679/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -405643.5312 - val_loss: -139256.2344\n",
            "Epoch 680/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -408560.7188 - val_loss: -139614.2812\n",
            "Epoch 681/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -410281.8750 - val_loss: -140358.5938\n",
            "Epoch 682/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -413409.0312 - val_loss: -140996.6562\n",
            "Epoch 683/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -413458.4375 - val_loss: -141915.9375\n",
            "Epoch 684/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -419915.0312 - val_loss: -142824.6875\n",
            "Epoch 685/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -418931.5938 - val_loss: -143140.6562\n",
            "Epoch 686/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -417847.6875 - val_loss: -143940.1094\n",
            "Epoch 687/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -423583.0000 - val_loss: -144440.4219\n",
            "Epoch 688/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -424488.7812 - val_loss: -144696.6875\n",
            "Epoch 689/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -428048.9375 - val_loss: -146191.9375\n",
            "Epoch 690/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -430479.0312 - val_loss: -146274.5312\n",
            "Epoch 691/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -429003.7500 - val_loss: -147492.2188\n",
            "Epoch 692/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -434298.1562 - val_loss: -147231.9375\n",
            "Epoch 693/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -435895.2500 - val_loss: -148217.0625\n",
            "Epoch 694/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -439855.5938 - val_loss: -148783.7656\n",
            "Epoch 695/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -441562.7812 - val_loss: -149518.1250\n",
            "Epoch 696/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -441153.6562 - val_loss: -149719.1719\n",
            "Epoch 697/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -449069.9375 - val_loss: -151062.8750\n",
            "Epoch 698/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -448372.2188 - val_loss: -151780.7656\n",
            "Epoch 699/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -450132.5312 - val_loss: -152362.1875\n",
            "Epoch 700/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -452239.3750 - val_loss: -152939.3906\n",
            "Epoch 701/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -456953.9375 - val_loss: -153426.1250\n",
            "Epoch 702/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 230514.3438 - val_loss: -153845.4062\n",
            "Epoch 703/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -457335.0625 - val_loss: -155001.1562\n",
            "Epoch 704/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -460982.7812 - val_loss: -155322.4062\n",
            "Epoch 705/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -462501.3125 - val_loss: -156514.7656\n",
            "Epoch 706/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -464931.0938 - val_loss: -157373.9531\n",
            "Epoch 707/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -468074.3125 - val_loss: -157831.5312\n",
            "Epoch 708/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -469941.5938 - val_loss: -158479.4844\n",
            "Epoch 709/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -472754.6250 - val_loss: -159285.8438\n",
            "Epoch 710/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -474221.9062 - val_loss: -159989.4688\n",
            "Epoch 711/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -477714.7500 - val_loss: -160541.7031\n",
            "Epoch 712/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -478518.3750 - val_loss: -161374.5938\n",
            "Epoch 713/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -483280.9375 - val_loss: -162437.0938\n",
            "Epoch 714/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -482660.4688 - val_loss: -163043.0312\n",
            "Epoch 715/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -489200.4688 - val_loss: -163789.1719\n",
            "Epoch 716/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -487373.6250 - val_loss: -164496.2188\n",
            "Epoch 717/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -492084.5312 - val_loss: -164984.4062\n",
            "Epoch 718/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -498729.3125 - val_loss: -166611.6719\n",
            "Epoch 719/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -495433.4688 - val_loss: -166832.9688\n",
            "Epoch 720/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -499523.6875 - val_loss: -168312.7188\n",
            "Epoch 721/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -499422.2500 - val_loss: -168222.0781\n",
            "Epoch 722/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -504539.5938 - val_loss: -168775.7500\n",
            "Epoch 723/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -505152.6250 - val_loss: -170035.9062\n",
            "Epoch 724/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -514314.2500 - val_loss: -170709.6406\n",
            "Epoch 725/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -509428.0938 - val_loss: -171638.5938\n",
            "Epoch 726/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -515220.6875 - val_loss: -172140.3750\n",
            "Epoch 727/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -518775.3125 - val_loss: -173255.6562\n",
            "Epoch 728/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -523620.0000 - val_loss: -173053.0938\n",
            "Epoch 729/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -524534.5000 - val_loss: -174991.6094\n",
            "Epoch 730/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -522135.5938 - val_loss: -175656.3438\n",
            "Epoch 731/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -525841.1250 - val_loss: -175809.3750\n",
            "Epoch 732/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -531238.5625 - val_loss: -176837.4219\n",
            "Epoch 733/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -541991.6875 - val_loss: -177909.7500\n",
            "Epoch 734/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -535506.5000 - val_loss: -178388.2188\n",
            "Epoch 735/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -541301.0625 - val_loss: -179818.6562\n",
            "Epoch 736/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -541003.3750 - val_loss: -180627.3750\n",
            "Epoch 737/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -545832.8125 - val_loss: -181157.1875\n",
            "Epoch 738/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -567744.1250 - val_loss: -181316.7188\n",
            "Epoch 739/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -550355.3125 - val_loss: -182814.5156\n",
            "Epoch 740/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -558017.6875 - val_loss: -183531.4688\n",
            "Epoch 741/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -556594.2500 - val_loss: -184420.7344\n",
            "Epoch 742/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -557983.9375 - val_loss: -185554.6719\n",
            "Epoch 743/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -562447.8750 - val_loss: -186487.4688\n",
            "Epoch 744/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -563253.0000 - val_loss: -187087.0312\n",
            "Epoch 745/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -564462.8125 - val_loss: -187157.0938\n",
            "Epoch 746/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -568891.1875 - val_loss: -188958.7969\n",
            "Epoch 747/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -571935.9375 - val_loss: -188955.5469\n",
            "Epoch 748/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -575662.5000 - val_loss: -190167.4062\n",
            "Epoch 749/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -577378.4375 - val_loss: -191124.1406\n",
            "Epoch 750/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -580006.2500 - val_loss: -191539.7969\n",
            "Epoch 751/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -592023.1875 - val_loss: -192545.9375\n",
            "Epoch 752/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -587714.5625 - val_loss: -193897.1719\n",
            "Epoch 753/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -587164.6250 - val_loss: -194314.3281\n",
            "Epoch 754/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -591376.5000 - val_loss: -195709.3438\n",
            "Epoch 755/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -597968.0625 - val_loss: -196165.8750\n",
            "Epoch 756/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -597555.6875 - val_loss: -197011.5312\n",
            "Epoch 757/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -599881.5625 - val_loss: -198271.1562\n",
            "Epoch 758/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -604854.2500 - val_loss: -198328.7344\n",
            "Epoch 759/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -609313.2500 - val_loss: -199412.0156\n",
            "Epoch 760/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -609922.1250 - val_loss: -200344.3281\n",
            "Epoch 761/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -613966.6250 - val_loss: -201290.6562\n",
            "Epoch 762/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -614511.4375 - val_loss: -202198.6562\n",
            "Epoch 763/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -618444.5000 - val_loss: -203139.3281\n",
            "Epoch 764/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -622390.1250 - val_loss: -203687.6562\n",
            "Epoch 765/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -629307.6875 - val_loss: -205003.3750\n",
            "Epoch 766/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -628780.1875 - val_loss: -206425.7812\n",
            "Epoch 767/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -632588.8125 - val_loss: -206549.4062\n",
            "Epoch 768/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -633800.7500 - val_loss: -207500.3125\n",
            "Epoch 769/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -636448.5000 - val_loss: -208491.7344\n",
            "Epoch 770/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -637674.9375 - val_loss: -208484.5312\n",
            "Epoch 771/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -642027.0000 - val_loss: -210660.5938\n",
            "Epoch 772/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -645414.7500 - val_loss: -210590.5000\n",
            "Epoch 773/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -649313.1875 - val_loss: -212030.5938\n",
            "Epoch 774/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -658840.8125 - val_loss: -212526.5469\n",
            "Epoch 775/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -659610.8125 - val_loss: -213745.5312\n",
            "Epoch 776/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -658345.3750 - val_loss: -213979.7188\n",
            "Epoch 777/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -660931.6250 - val_loss: -215649.0312\n",
            "Epoch 778/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -665396.5625 - val_loss: -216387.1719\n",
            "Epoch 779/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -667696.6875 - val_loss: -217441.2500\n",
            "Epoch 780/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -671604.9375 - val_loss: -218606.7812\n",
            "Epoch 781/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -679094.8125 - val_loss: -218752.2188\n",
            "Epoch 782/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -708930.5000 - val_loss: -220126.8438\n",
            "Epoch 783/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -678332.3125 - val_loss: -220819.2188\n",
            "Epoch 784/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -684828.6875 - val_loss: -222293.3281\n",
            "Epoch 785/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -688070.5000 - val_loss: -222299.2969\n",
            "Epoch 786/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -690536.4375 - val_loss: -224902.3438\n",
            "Epoch 787/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -698023.5000 - val_loss: -208632.5312\n",
            "Epoch 788/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 593426.0000 - val_loss: -227192.9219\n",
            "Epoch 789/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -707960.8125 - val_loss: -227025.9062\n",
            "Epoch 790/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -708038.5625 - val_loss: -228807.0312\n",
            "Epoch 791/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -707687.0625 - val_loss: -229121.6719\n",
            "Epoch 792/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -708253.1250 - val_loss: -229249.1719\n",
            "Epoch 793/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -712502.4375 - val_loss: -230763.6719\n",
            "Epoch 794/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -719091.4375 - val_loss: -231175.8281\n",
            "Epoch 795/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -723650.5000 - val_loss: -232205.7188\n",
            "Epoch 796/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -727715.5625 - val_loss: -232141.2031\n",
            "Epoch 797/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -728061.1250 - val_loss: -234630.1562\n",
            "Epoch 798/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -731128.8125 - val_loss: -234711.5938\n",
            "Epoch 799/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -734217.7500 - val_loss: -236399.4531\n",
            "Epoch 800/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -740339.8125 - val_loss: -236972.6562\n",
            "Epoch 801/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -736988.5000 - val_loss: -238121.2188\n",
            "Epoch 802/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -742732.6875 - val_loss: -239689.2031\n",
            "Epoch 803/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -745138.8750 - val_loss: -239726.4688\n",
            "Epoch 804/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -750674.8750 - val_loss: -240597.5312\n",
            "Epoch 805/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -755377.0625 - val_loss: -242292.0469\n",
            "Epoch 806/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -760454.4375 - val_loss: -243168.9062\n",
            "Epoch 807/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -760418.9375 - val_loss: -244137.7188\n",
            "Epoch 808/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -765816.1875 - val_loss: -244781.8750\n",
            "Epoch 809/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -767314.4375 - val_loss: -245289.0781\n",
            "Epoch 810/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -772380.8125 - val_loss: -236665.1562\n",
            "Epoch 811/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -776884.4375 - val_loss: -218137.2500\n",
            "Epoch 812/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -782218.1875 - val_loss: -249925.2031\n",
            "Epoch 813/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -795284.1250 - val_loss: -251041.5781\n",
            "Epoch 814/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -790368.8125 - val_loss: -250034.7188\n",
            "Epoch 815/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -792573.1875 - val_loss: -250952.4062\n",
            "Epoch 816/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -825688.1875 - val_loss: -252466.5312\n",
            "Epoch 817/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -800132.3750 - val_loss: -252497.4688\n",
            "Epoch 818/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -803803.5000 - val_loss: -254258.9219\n",
            "Epoch 819/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -807252.0625 - val_loss: -254245.4062\n",
            "Epoch 820/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -810594.3125 - val_loss: -243701.0938\n",
            "Epoch 821/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -809619.4375 - val_loss: -256211.0938\n",
            "Epoch 822/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -814249.7500 - val_loss: -258509.2500\n",
            "Epoch 823/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -820026.0000 - val_loss: -259686.3281\n",
            "Epoch 824/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -825102.1250 - val_loss: -259917.4219\n",
            "Epoch 825/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -827608.1250 - val_loss: -262016.1562\n",
            "Epoch 826/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -832201.2500 - val_loss: -262055.8438\n",
            "Epoch 827/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -836288.1875 - val_loss: -263856.0000\n",
            "Epoch 828/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -836173.0625 - val_loss: -263866.8438\n",
            "Epoch 829/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -845944.0625 - val_loss: -265902.6875\n",
            "Epoch 830/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -846577.9375 - val_loss: -266774.7188\n",
            "Epoch 831/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -851249.5000 - val_loss: -266868.8438\n",
            "Epoch 832/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -854363.7500 - val_loss: -269329.6250\n",
            "Epoch 833/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -856683.5625 - val_loss: -270554.7500\n",
            "Epoch 834/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -892641.5625 - val_loss: -270353.0000\n",
            "Epoch 835/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -867737.8125 - val_loss: -272767.9375\n",
            "Epoch 836/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -871987.6250 - val_loss: -274033.9375\n",
            "Epoch 837/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -872383.6875 - val_loss: -275348.8125\n",
            "Epoch 838/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -876558.0625 - val_loss: -275943.3125\n",
            "Epoch 839/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -881987.2500 - val_loss: -275546.4375\n",
            "Epoch 840/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -885486.0000 - val_loss: -276327.6562\n",
            "Epoch 841/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -888268.1250 - val_loss: -277602.2500\n",
            "Epoch 842/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -894389.8125 - val_loss: -280954.4375\n",
            "Epoch 843/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -894289.5000 - val_loss: -277992.9375\n",
            "Epoch 844/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -896319.2500 - val_loss: -281526.5938\n",
            "Epoch 845/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -905386.1250 - val_loss: -282721.7500\n",
            "Epoch 846/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -907816.6250 - val_loss: -286645.3125\n",
            "Epoch 847/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -914289.2500 - val_loss: -285235.6875\n",
            "Epoch 848/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -920936.8125 - val_loss: -286182.7188\n",
            "Epoch 849/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -921761.1875 - val_loss: -287175.8750\n",
            "Epoch 850/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -926998.0000 - val_loss: -289660.3438\n",
            "Epoch 851/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -929572.9375 - val_loss: -289665.8438\n",
            "Epoch 852/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -934055.5000 - val_loss: -289799.3438\n",
            "Epoch 853/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -937478.3125 - val_loss: -291903.6875\n",
            "Epoch 854/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -942931.3750 - val_loss: -293687.1562\n",
            "Epoch 855/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -944730.5000 - val_loss: -294530.1562\n",
            "Epoch 856/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -952602.1875 - val_loss: -293225.5625\n",
            "Epoch 857/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -953564.9375 - val_loss: -295295.2812\n",
            "Epoch 858/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -960484.4375 - val_loss: -293464.9375\n",
            "Epoch 859/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -827759.0625 - val_loss: -296468.6250\n",
            "Epoch 860/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -964837.8750 - val_loss: -296378.8438\n",
            "Epoch 861/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -969564.9375 - val_loss: -299497.5312\n",
            "Epoch 862/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -977390.5000 - val_loss: -299248.5625\n",
            "Epoch 863/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -976485.8750 - val_loss: -300478.1250\n",
            "Epoch 864/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -979938.4375 - val_loss: -300048.1875\n",
            "Epoch 865/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -988848.1875 - val_loss: -301814.0000\n",
            "Epoch 866/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -992587.1875 - val_loss: -303277.5625\n",
            "Epoch 867/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -991175.5000 - val_loss: -304553.8438\n",
            "Epoch 868/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1009243.8750 - val_loss: -306244.2812\n",
            "Epoch 869/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1014719.8750 - val_loss: -306540.3125\n",
            "Epoch 870/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1009015.1875 - val_loss: -309991.6875\n",
            "Epoch 871/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1012686.3125 - val_loss: -309931.8125\n",
            "Epoch 872/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1020525.8750 - val_loss: -311626.3438\n",
            "Epoch 873/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1023843.5625 - val_loss: -312269.4375\n",
            "Epoch 874/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1029351.2500 - val_loss: -312212.8125\n",
            "Epoch 875/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1032169.8750 - val_loss: -313744.2500\n",
            "Epoch 876/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1035094.6250 - val_loss: -314034.2188\n",
            "Epoch 877/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1041176.1250 - val_loss: -315962.2188\n",
            "Epoch 878/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1044931.5625 - val_loss: -316704.0000\n",
            "Epoch 879/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1089137.2500 - val_loss: -296069.9375\n",
            "Epoch 880/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1054326.6250 - val_loss: -319054.8125\n",
            "Epoch 881/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1060348.6250 - val_loss: -321919.7188\n",
            "Epoch 882/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1110736.8750 - val_loss: -321761.9375\n",
            "Epoch 883/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1071539.0000 - val_loss: -325230.1875\n",
            "Epoch 884/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1076323.0000 - val_loss: -326199.1562\n",
            "Epoch 885/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1074296.3750 - val_loss: -326128.5938\n",
            "Epoch 886/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1083750.3750 - val_loss: -325587.4688\n",
            "Epoch 887/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1084630.6250 - val_loss: -328952.1875\n",
            "Epoch 888/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1089526.8750 - val_loss: -329940.2500\n",
            "Epoch 889/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1094446.5000 - val_loss: -330565.5000\n",
            "Epoch 890/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1101358.2500 - val_loss: -332553.8125\n",
            "Epoch 891/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1111314.8750 - val_loss: -333256.5625\n",
            "Epoch 892/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1108270.7500 - val_loss: -335052.7812\n",
            "Epoch 893/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1113458.3750 - val_loss: -334506.0000\n",
            "Epoch 894/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1121056.7500 - val_loss: -336265.1250\n",
            "Epoch 895/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1121718.5000 - val_loss: -337349.4688\n",
            "Epoch 896/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1131025.6250 - val_loss: -339487.5938\n",
            "Epoch 897/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1131862.6250 - val_loss: -341125.5625\n",
            "Epoch 898/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1147944.3750 - val_loss: -342454.2188\n",
            "Epoch 899/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1151711.2500 - val_loss: -343449.3438\n",
            "Epoch 900/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1149723.6250 - val_loss: -344632.0000\n",
            "Epoch 901/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1156793.7500 - val_loss: -344803.2188\n",
            "Epoch 902/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1159852.1250 - val_loss: -347336.1875\n",
            "Epoch 903/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1166706.6250 - val_loss: -347834.6875\n",
            "Epoch 904/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1173674.7500 - val_loss: -350075.8125\n",
            "Epoch 905/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1175932.0000 - val_loss: -351690.4688\n",
            "Epoch 906/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -1181715.5000 - val_loss: -352631.2188\n",
            "Epoch 907/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1184334.2500 - val_loss: -353810.5938\n",
            "Epoch 908/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1234690.3750 - val_loss: -354663.5625\n",
            "Epoch 909/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1195611.2500 - val_loss: -355667.1562\n",
            "Epoch 910/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1206851.5000 - val_loss: -357569.6875\n",
            "Epoch 911/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1206366.3750 - val_loss: -358620.5000\n",
            "Epoch 912/1000\n",
            "38/38 [==============================] - 0s 12ms/step - loss: -1212992.0000 - val_loss: -360000.8438\n",
            "Epoch 913/1000\n",
            "38/38 [==============================] - 1s 14ms/step - loss: -1218898.7500 - val_loss: -360754.7188\n",
            "Epoch 914/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1223533.6250 - val_loss: -362867.6875\n",
            "Epoch 915/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1226756.1250 - val_loss: -364539.5938\n",
            "Epoch 916/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1233998.3750 - val_loss: -365362.3750\n",
            "Epoch 917/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1239742.1250 - val_loss: -367141.0938\n",
            "Epoch 918/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1248678.3750 - val_loss: -367120.6875\n",
            "Epoch 919/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1252061.6250 - val_loss: -370301.9062\n",
            "Epoch 920/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1255987.5000 - val_loss: -370190.5938\n",
            "Epoch 921/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1281225.0000 - val_loss: -372602.0000\n",
            "Epoch 922/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1266482.2500 - val_loss: -373071.0000\n",
            "Epoch 923/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1272747.0000 - val_loss: -375296.5000\n",
            "Epoch 924/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1281947.3750 - val_loss: -376977.0938\n",
            "Epoch 925/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1288788.5000 - val_loss: -377193.4375\n",
            "Epoch 926/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1291786.7500 - val_loss: -380568.4062\n",
            "Epoch 927/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1298724.3750 - val_loss: -380266.4688\n",
            "Epoch 928/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1301371.3750 - val_loss: -382695.1875\n",
            "Epoch 929/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1306788.0000 - val_loss: -385573.8125\n",
            "Epoch 930/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1314329.1250 - val_loss: -385692.1250\n",
            "Epoch 931/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1331794.6250 - val_loss: -388799.6875\n",
            "Epoch 932/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1329111.3750 - val_loss: -388139.6875\n",
            "Epoch 933/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1327753.6250 - val_loss: -389084.1562\n",
            "Epoch 934/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1385769.8750 - val_loss: -386493.5625\n",
            "Epoch 935/1000\n",
            "38/38 [==============================] - 0s 10ms/step - loss: -1342018.8750 - val_loss: -394212.9062\n",
            "Epoch 936/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1374177.6250 - val_loss: -394859.5312\n",
            "Epoch 937/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1355471.5000 - val_loss: -394525.5625\n",
            "Epoch 938/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1363850.6250 - val_loss: -397019.8125\n",
            "Epoch 939/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -1369751.0000 - val_loss: -399694.6875\n",
            "Epoch 940/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1370109.6250 - val_loss: -399752.2812\n",
            "Epoch 941/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -1381773.1250 - val_loss: -400329.0938\n",
            "Epoch 942/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1385224.7500 - val_loss: -403849.8125\n",
            "Epoch 943/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1390887.5000 - val_loss: -403696.7500\n",
            "Epoch 944/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1393945.5000 - val_loss: -407318.6875\n",
            "Epoch 945/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1404483.6250 - val_loss: -407333.0625\n",
            "Epoch 946/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1409698.2500 - val_loss: -408877.2188\n",
            "Epoch 947/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1419309.6250 - val_loss: -410113.5312\n",
            "Epoch 948/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -1422371.6250 - val_loss: -413028.5625\n",
            "Epoch 949/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -1432739.8750 - val_loss: -413505.6875\n",
            "Epoch 950/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -1455602.3750 - val_loss: -417262.3750\n",
            "Epoch 951/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: -1440404.3750 - val_loss: -417913.0938\n",
            "Epoch 952/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1449800.5000 - val_loss: -418576.0625\n",
            "Epoch 953/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1458474.8750 - val_loss: -421625.6875\n",
            "Epoch 954/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1462338.5000 - val_loss: -423445.3125\n",
            "Epoch 955/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1468062.6250 - val_loss: -424710.9375\n",
            "Epoch 956/1000\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -1473660.3750 - val_loss: -426006.0938\n",
            "Epoch 957/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1487251.1250 - val_loss: -428117.1562\n",
            "Epoch 958/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1489409.7500 - val_loss: -427246.3438\n",
            "Epoch 959/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1512684.3750 - val_loss: -431286.4375\n",
            "Epoch 960/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1499569.7500 - val_loss: -432947.3438\n",
            "Epoch 961/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1509316.6250 - val_loss: -434738.9375\n",
            "Epoch 962/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1515861.0000 - val_loss: -436268.1875\n",
            "Epoch 963/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1522484.7500 - val_loss: -437641.0938\n",
            "Epoch 964/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1530515.0000 - val_loss: -441431.5938\n",
            "Epoch 965/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1536389.2500 - val_loss: -442580.0000\n",
            "Epoch 966/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1546429.0000 - val_loss: -445401.5625\n",
            "Epoch 967/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1549881.0000 - val_loss: -446421.3125\n",
            "Epoch 968/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1561470.5000 - val_loss: -447583.5000\n",
            "Epoch 969/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1567139.2500 - val_loss: -449817.0625\n",
            "Epoch 970/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1631559.0000 - val_loss: -451497.1875\n",
            "Epoch 971/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1583607.0000 - val_loss: -453936.3438\n",
            "Epoch 972/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1587319.8750 - val_loss: -454932.0625\n",
            "Epoch 973/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1598844.5000 - val_loss: -458533.5000\n",
            "Epoch 974/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1604712.3750 - val_loss: -458683.7500\n",
            "Epoch 975/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1607313.5000 - val_loss: -461247.9062\n",
            "Epoch 976/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1620599.0000 - val_loss: -465116.3438\n",
            "Epoch 977/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1653477.2500 - val_loss: -465173.0000\n",
            "Epoch 978/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1634607.2500 - val_loss: -467522.0938\n",
            "Epoch 979/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1639654.0000 - val_loss: -469973.9375\n",
            "Epoch 980/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1648903.0000 - val_loss: -470875.8438\n",
            "Epoch 981/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1656284.6250 - val_loss: -474173.9062\n",
            "Epoch 982/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1664948.2500 - val_loss: -476526.6875\n",
            "Epoch 983/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1677001.3750 - val_loss: -478416.5938\n",
            "Epoch 984/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1680364.0000 - val_loss: -479493.9062\n",
            "Epoch 985/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1685596.7500 - val_loss: -481185.5938\n",
            "Epoch 986/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1697624.8750 - val_loss: -483810.3125\n",
            "Epoch 987/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1704593.3750 - val_loss: -487249.4062\n",
            "Epoch 988/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1713368.2500 - val_loss: -487597.6875\n",
            "Epoch 989/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1717694.6250 - val_loss: -491179.9375\n",
            "Epoch 990/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1726916.0000 - val_loss: -491993.3438\n",
            "Epoch 991/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1734352.5000 - val_loss: -496051.0000\n",
            "Epoch 992/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1745678.0000 - val_loss: -496812.3125\n",
            "Epoch 993/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1750667.0000 - val_loss: -499146.6875\n",
            "Epoch 994/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1758991.2500 - val_loss: -502072.5000\n",
            "Epoch 995/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1767494.3750 - val_loss: -502984.3438\n",
            "Epoch 996/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1776461.0000 - val_loss: -503079.8438\n",
            "Epoch 997/1000\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -1784313.6250 - val_loss: -507546.4062\n",
            "Epoch 998/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -1789472.3750 - val_loss: -509265.9062\n",
            "Epoch 999/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1805109.8750 - val_loss: -512510.2500\n",
            "Epoch 1000/1000\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1812769.0000 - val_loss: -513018.6562\n",
            "1/1 [==============================] - 0s 59ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate & Visualize"
      ],
      "metadata": {
        "id": "wkkkQCM_3vmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65yWPMBCRviy",
        "outputId": "3c0516f4-d8ed-4892-d92c-ccceba9d6223"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the encoder and decoder\n",
        "encoder.save('/mnt/drive/MyDrive/encoder.h5')\n",
        "decoder.save('/mnt/drive/MyDrive/decoder.h5')\n",
        "\n",
        "# Save the entire VAE model\n",
        "vae.save('/mnt/drive/MyDrive/vae.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD8L0ol1TFWb",
        "outputId": "ab5eb69a-a7fd-47d3-fa83-9f85c760b208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved decoder model\n",
        "decoder = load_model('/content/drive/MyDrive/decoder.h5')\n",
        "\n",
        "# Function to generate an image from random latent vector\n",
        "def generate_image(decoder, latent_dim=8):\n",
        "    # Sample a random point from the latent space\n",
        "    latent_sample = np.random.normal(size=(1, latent_dim)).astype('float32')\n",
        "\n",
        "    # Decode the latent vector to get the generated image\n",
        "    generated_image = decoder.predict(latent_sample)\n",
        "\n",
        "    return generated_image.reshape(8, 8)  # Reshape to original image dimensions\n",
        "\n",
        "# Generate an image\n",
        "generated_image = generate_image(decoder)\n",
        "\n",
        "# Plot the generated image\n",
        "plt.imshow(generated_image, cmap='binary')\n",
        "plt.title('Generated Dungeon Layout')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "E5bPNqi735tS",
        "outputId": "9409ad57-1535-4eaa-f542-f8e460afc07b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 61ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZV0lEQVR4nO3de3jO9/3H8VdORHInVEQpFYpSpGuXTs1hCUIINQx1mFOvSzNT4jQ1vVTXWo12xuV0MbvUVMqwWevQVlXVTGebUOHSRgRzmLM4xKGSz++P/vL+ud1RcYvfTft8XFeuq7735/7mfR/ked/f+xsNcs45AQAgKTjQAwAA7h1EAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAfecmjVrasCAAYEeA/hOIgqlKDc3Vy+88IIeffRRRUREKCIiQg0aNNCQIUP0+eefB3q8UrVmzRq98sorAZ0hKCjIvkJDQ1WxYkUlJCQoPT1du3fvDuhs94NXXnlFQUFBOnnyZKBHuSMZGRmaNm1aoMf41ggN9ADfFqtWrdKzzz6r0NBQ9enTR9/73vcUHBysPXv26M9//rPmzJmj3NxcxcXFBXrUUrFmzRrNmjUr4GFo06aN+vXrJ+ec8vLytGPHDi1cuFCzZ8/W5MmTNXLkyIDOh7svIyNDWVlZGj58eKBH+VYgCqUgJydHPXv2VFxcnNavX6+qVat6XT558mTNnj1bwcH37huzixcvKjIyMtBj3LZHH31UP/3pT722/eY3v9EzzzyjUaNGqX79+kpNTQ3QdMD95979KXUfmTJlii5evKgFCxb4BEGSQkNDNWzYMD388MNe2/fs2aNu3bqpYsWKCg8P11NPPaV3333Xa81bb72loKAgbd68WSNHjlRsbKwiIyPVpUsXnThxwud7rV27Vi1atFBkZKSioqLUoUMH7dq1y2vNgAED5PF4lJOTo9TUVEVFRalPnz6SpE2bNql79+6qUaOGypYtq4cfflgjRozQpUuXvK4/a9YsSd6HcIoUFhZq2rRpatiwocLDw/Xggw8qLS1NZ86c8ZrDOaeJEyeqevXqioiIUMuWLX1m9UdMTIyWLFmi0NBQ/frXv/a5L/fv3++1/pNPPlFQUJA++eQT25aUlKRGjRpp9+7datmypSIiIlStWjVNmTLF5/sdOHBAnTp1UmRkpCpXrqwRI0bogw8+8NmnJP3jH/9Qu3btVL58eUVERCgxMVGbN2/22WdmZqbat2+v6OhoeTwetW7dWp999pnXmtt9bvjj9OnTGj16tOLj4+XxeBQdHa327dtrx44dtubChQuKjIxUenq6z/UPHTqkkJAQTZo0ybbt27dP3bt3V8WKFRUREaEmTZpo9erVxd62Wz1WSUlJWr16tQ4cOGDPw5o1a5bKbf+u4p1CKVi1apXq1Kmjp59+usTX2bVrl5o1a6Zq1app7NixioyM1J/+9Cd17txZK1asUJcuXbzWDx06VA888IAmTJig/fv3a9q0aXrhhRe0dOlSW7No0SL1799fKSkpmjx5svLz8zVnzhw1b95cmZmZXn9Zrl27ppSUFDVv3lxvvvmmIiIiJEnLli1Tfn6+Bg8erJiYGG3dulUzZszQoUOHtGzZMklSWlqajhw5onXr1mnRokU+ty0tLU1vvfWWBg4cqGHDhik3N1czZ85UZmamNm/erLCwMEnSyy+/rIkTJyo1NVWpqanatm2b2rZtq6tXr5b4fryZGjVqKDExURs2bNC5c+cUHR192/s4c+aM2rVrp65du6pHjx5avny5XnzxRcXHx6t9+/aSvn6H1apVKx09elTp6emqUqWKMjIytGHDBp/9ffzxx2rfvr0SEhI0YcIEBQcHa8GCBWrVqpU2bdqkxo0bS/r6udGiRQtFR0drzJgxCgsL09y5c5WUlKSNGzf6PM9K8tzw1759+7Ry5Up1795dtWrV0rFjxzR37lwlJiZq9+7deuihh+TxeNSlSxctXbpUU6dOVUhIiF3/nXfekXPOXnQcO3ZMTZs2VX5+voYNG6aYmBgtXLhQnTp10vLly32e97fy0ksvKS8vT4cOHdLvfvc7SZLH47nj2/2d5nBH8vLynCTXuXNnn8vOnDnjTpw4YV/5+fl2WevWrV18fLy7fPmybSssLHRNmzZ1devWtW0LFixwklxycrIrLCy07SNGjHAhISHu7Nmzzjnnzp8/7ypUqOAGDRrkNcN///tfV758ea/t/fv3d5Lc2LFjfWa+fsYikyZNckFBQe7AgQO2bciQIa64p8+mTZucJLd48WKv7e+//77X9uPHj7syZcq4Dh06eN2ucePGOUmuf//+Pvu+kSQ3ZMiQm16enp7uJLkdO3Y45/7vvszNzfVat2HDBifJbdiwwbYlJiY6Se6Pf/yjbbty5YqrUqWK+8lPfmLbfvvb3zpJbuXKlbbt0qVLrn79+l77LCwsdHXr1nUpKSletzc/P9/VqlXLtWnTxrZ17tzZlSlTxuXk5Ni2I0eOuKioKPejH/3ItpX0uXEzEyZMcJLciRMnbrrm8uXLrqCgwGtbbm6uK1u2rHv11Vdt2wcffOAkubVr13qtffzxx11iYqL9efjw4U6S27Rpk207f/68q1WrlqtZs6Z9r9t5rDp06ODi4uK+8bai5Dh8dIfOnTsnqfhXJ0lJSYqNjbWvokMup0+f1scff6wePXro/PnzOnnypE6ePKlTp04pJSVF2dnZOnz4sNe+nn/+ea9DNC1atFBBQYEOHDggSVq3bp3Onj2rXr162f5OnjypkJAQPf3008W+ch08eLDPtnLlytl/X7x4USdPnlTTpk3lnFNmZuYt749ly5apfPnyatOmjdccCQkJ8ng8NsdHH32kq1evaujQoV63qzQ/LCx6TM6fP+/39a//vKJMmTJq3Lix9u3bZ9vef/99VatWTZ06dbJt4eHhGjRokNe+tm/fruzsbPXu3VunTp2y++XixYtq3bq1Pv30UxUWFqqgoEAffvihOnfurEceecSuX7VqVfXu3Vt/+9vf7DlX5FbPjTtRtmxZ+yysoKBAp06dksfjUb169bRt2zZbl5ycrIceekiLFy+2bVlZWfr888+97sM1a9aocePGat68uW3zeDx6/vnntX//fs4auwdw+OgORUVFSfr6uOqN5s6dq/Pnz+vYsWNefzH27t0r55zGjx+v8ePHF7vf48ePq1q1avbnGjVqeF3+wAMPSJIdp8/OzpYktWrVqtj93Xj4JDQ0VNWrV/dZd/DgQb388st69913fT4DyMvLK3bf18vOzlZeXp4qV65c7OXHjx+XJPuBVbduXa/LY2Nj7bbdqaLHpOgxul3Vq1f3+mErfX2/X3968YEDB1S7dm2fdXXq1PH6c9Hj079//5t+v7y8PF25ckX5+fmqV6+ez+WPPfaYCgsL9Z///EcNGza07bd6btyJwsJCTZ8+XbNnz1Zubq4KCgrsspiYGPvv4OBg9enTR3PmzFF+fr4iIiK0ePFihYeHq3v37rbuwIEDxR5mfeyxx+zyRo0a3fHc8B9RuEPly5dX1apVlZWV5XNZ0ZP/xg/LCgsLJUmjR49WSkpKsfu98YfK9cdpr+f+9/+mWrTPRYsWqUqVKj7rQkO9H+rrXwEWKSgoUJs2bXT69Gm9+OKLql+/viIjI3X48GENGDDAvsc3KSwsVOXKlb1eMV4vNjb2lvsoLVlZWQoJCVGtWrUkyecHd5Hrf9Bd71b3+e0ouu/eeOMNPfHEE8Wu8Xg8unLlym3vuzTnvNHrr7+u8ePH67nnntNrr72mihUrKjg4WMOHD/d5PvTr109vvPGGVq5cqV69eikjI0MdO3ZU+fLlb/v73u5jhdJDFEpBhw4dNH/+fG3dutU+LPwmRYcFwsLClJycXCoz1K5dW5JUuXJlv/e5c+dOffnll1q4cKH69etn29etW+ez9mZ/aWvXrq2PPvpIzZo18zoUdaOi39fIzs72Okxy4sSJUnmFe/DgQW3cuFE//OEP7Z1C0Svos2fPeq29k8MscXFx2r17t5xzXvfJ3r17vdYVPT7R0dHf+PjExsYqIiJCX3zxhc9le/bsUXBwsM9ZbHfT8uXL1bJlS/3hD3/w2n727FlVqlTJa1ujRo305JNPavHixapevboOHjyoGTNmeK2Ji4u76W0ruly6vcfqZs9F+IfPFErBmDFjFBERoeeee07Hjh3zufzGV2yVK1dWUlKS5s6dq6NHj/qs9+d0wpSUFEVHR+v111/XV1995dc+i15xXj+vc07Tp0/3WVv0Ow03/qXt0aOHCgoK9Nprr/lc59q1a7Y+OTlZYWFhmjFjhtf3K43fTD19+rR69eqlgoICvfTSS7a96Afzp59+atsKCgo0b948v79XSkqKDh8+7HUq8eXLl/X73//ea11CQoJq166tN998s9hDjUWPT0hIiNq2bau//vWvXu8wjx07poyMDDVv3tyvM6n8FRIS4vP8XbZsmc9nXkX69u2rDz/8UNOmTVNMTIydpVUkNTVVW7du1ZYtW2zbxYsXNW/ePNWsWVMNGjSQdHuPVWRkZIkObaJkeKdQCurWrauMjAz16tVL9erVs99ods4pNzdXGRkZCg4O9jqGP2vWLDVv3lzx8fEaNGiQHnnkER07dkxbtmzRoUOHvM4DL4no6GjNmTNHffv21fe//3317NlTsbGxOnjwoFavXq1mzZpp5syZ37iP+vXrq3bt2ho9erQOHz6s6OhorVixothX7gkJCZKkYcOGKSUlRSEhIerZs6cSExOVlpamSZMmafv27Wrbtq3CwsKUnZ2tZcuWafr06erWrZtiY2M1evRoTZo0SR07dlRqaqoyMzO1du1an1eg3+TLL7/U22+/Leeczp07px07dmjZsmW6cOGCpk6dqnbt2tnahg0bqkmTJvrlL3+p06dPq2LFilqyZImuXbtW4u93o7S0NM2cOVO9evVSenq6qlatasfSpf97FRscHKz58+erffv2atiwoQYOHKhq1arp8OHD2rBhg6Kjo/Xee+9JkiZOnKh169apefPm+vnPf67Q0FDNnTtXV65cKfb3JO7U1KlT7ZTkIsHBwRo3bpw6duyoV199VQMHDlTTpk21c+dOLV682Ovd3fV69+6tMWPG6C9/+YsGDx5spx8XGTt2rN555x21b99ew4YNU8WKFbVw4ULl5uZqxYoVdkjzdh6rhIQELV26VCNHjtQPfvADeTwePfPMM6V073wHBeScp2+pvXv3usGDB7s6deq48PBwV65cOVe/fn33s5/9zG3fvt1nfU5OjuvXr5+rUqWKCwsLc9WqVXMdO3Z0y5cvtzVFp+b985//9LpucafmFW1PSUlx5cuXd+Hh4a527dpuwIAB7l//+pet6d+/v4uMjCz2NuzevdslJyc7j8fjKlWq5AYNGuR27NjhJLkFCxbYumvXrrmhQ4e62NhYFxQU5HN66rx581xCQoIrV66ci4qKcvHx8W7MmDHuyJEjtqagoMD96le/clWrVnXlypVzSUlJLisry8XFxZX4lNSir+DgYFehQgX35JNPuvT0dLdr165ir5OTk+OSk5Nd2bJl3YMPPujGjRvn1q1bV+wpqQ0bNvS5fv/+/X1Of9y3b5/r0KGDK1eunIuNjXWjRo1yK1ascJLcZ5995rU2MzPTde3a1cXExLiyZcu6uLg416NHD7d+/Xqvddu2bXMpKSnO4/G4iIgI17JlS/f3v//da83tPjduVHRKanFfISEhzrmvT0kdNWqUPUbNmjVzW7ZscYmJiV6nml4vNTXVSfKZt0hOTo7r1q2bq1ChggsPD3eNGzd2q1atKnZdSR6rCxcuuN69e7sKFSo4SZyeeoeCnCuFT6MAeJk2bZpGjBihQ4cOeZ1F9l3QpUsX7dy50+dzFdwf+EwBuEPX/xMg0tefKcydO1d169b9zgXh6NGjWr16tfr27RvoUeAnPlMA7lDXrl1Vo0YNPfHEE8rLy9Pbb7+tPXv23PS03G+j3Nxcbd68WfPnz1dYWJjS0tICPRL8RBSAO5SSkqL58+dr8eLFKigoUIMGDbRkyRI9++yzgR7t/83GjRs1cOBA1ahRQwsXLiz2d2Vwf+AzBQCA4TMFAIAhCgAAU+LPFNavX38357hr7td/W71JkyaBHsFvbdq0CfQIfvmmf5bjXnbj/5jpfpGTkxPoEfx28ODBQI/gl6SkpFuu4Z0CAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAATGiJF4aWeOk9pVOnToEewS9PPfVUoEfwW/Xq1QM9gl+ysrICPYJfgoKCAj2CX+rVqxfoEfy2YcOGQI9w1/BOAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAACa0pAtr1KhxN+e4a3784x8HegS/zJw5M9Aj+O3f//53oEfwy6xZswI9gl/u1+dKu3btAj2C306dOhXoEe4a3ikAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgAlyzrlAD3E33a83r7CwMNAj+C00NDTQI3ynNGnSJNAj+OXIkSOBHsFv7733XqBH8Mvjjz9+yzW8UwAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAJLenCS5cu3c057pq9e/cGegS/xMfHB3oEvyUnJwd6BL+EhYUFegS/XL58OdAj+OWLL74I9Ah+O336dKBHuGt4pwAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAAATWtKF5cqVu5tz3DWNGjUK9AjfOfHx8YEewS9TpkwJ9Ah+CQsLC/QIfvnqq68CPYLfypQpE+gR7hreKQAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMCEBnqAuy0oKCjQI/jFORfoEfxWqVKlQI/gl1/84heBHsEvrVq1CvQIfgkOvn9fk0ZFRQV6hLvm/n1UAACljigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDABDnnXEkWlnDZPed+nft+dubMmUCP4JeYmJhAj+CXq1evBnoEv1y7di3QI/jtfv25EhkZecs1vFMAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCACXLOuUAPAQC4N/BOAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBg/gcDxb02gxIspQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Correction Algorithm\n"
      ],
      "metadata": {
        "id": "3WHXKIlZ9IAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_binary(image, threshold=0.5, invert=False):\n",
        "    # Apply thresholding\n",
        "    binary_image = np.where(image > threshold, 1.0, 0.0)\n",
        "\n",
        "    # Invert colors if specified\n",
        "    if invert:\n",
        "        binary_image = 1.0 - binary_image\n",
        "\n",
        "    return binary_image\n",
        "\n",
        "# Convert grayscale image to binary and invert colors\n",
        "binary_threshold = 0.5  # Adjust this threshold as needed\n",
        "invert_colors = True  # Set to True to invert colors\n",
        "binary_image = convert_to_binary(generated_image, threshold=binary_threshold, invert=invert_colors)\n",
        "\n",
        "# Plot the generated binary image with inverted colors\n",
        "plt.imshow(binary_image, cmap='binary')\n",
        "plt.title('Generated Dungeon Layout (Binary, Inverted)')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "i7eMk9hW9L1k",
        "outputId": "91005636-182e-4c8b-fdee-8f86d9fd5a58"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "nkiRTBI2C1HB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(decoder, num_samples=304, latent_dim=8):\n",
        "    latent_samples = np.random.normal(size=(num_samples, latent_dim)).astype('float32')\n",
        "    generated_samples = decoder.predict(latent_samples)\n",
        "    return generated_samples\n",
        "\n",
        "num_samples = test_data.shape[0]  # Match the number of test samples\n",
        "generated_samples = generate_samples(decoder, num_samples)\n"
      ],
      "metadata": {
        "id": "e7FPbM9ADGDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "170bbeac-533b-470b-ec63-f9751b60e191"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 13ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_binary(images):\n",
        "    return (images > 0.5).astype(int)\n",
        "\n",
        "binary_generated_samples = convert_to_binary(generated_samples)\n",
        "binary_test_data = convert_to_binary(test_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "bU8gzW6-HH1h"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install prdc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9xK7Zw2IAeH",
        "outputId": "a48247ee-8293-452c-bfd3-8319228db6e8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting prdc\n",
            "  Downloading prdc-0.2-py3-none-any.whl (6.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from prdc) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from prdc) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from prdc) (1.11.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from prdc) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->prdc) (3.5.0)\n",
            "Installing collected packages: prdc\n",
            "Successfully installed prdc-0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prdc import compute_prdc\n",
        "\n",
        "def compute_precision_recall(real_images, generated_images):\n",
        "    real_images_flat = real_images.reshape((real_images.shape[0], -1))  # Flatten images\n",
        "    generated_images_flat = generated_images.reshape((generated_images.shape[0], -1))  # Flatten images\n",
        "\n",
        "    metrics = compute_prdc(real_features=real_images_flat, fake_features=generated_images_flat, nearest_k=5)\n",
        "    return metrics\n",
        "\n",
        "precision_recall_metrics = compute_precision_recall(binary_test_data, binary_generated_samples)\n",
        "print(f\"Precision: {precision_recall_metrics['precision']}\")\n",
        "print(f\"Recall: {precision_recall_metrics['recall']}\")\n",
        "\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "def measure_diversity(images):\n",
        "    images_flat = images.reshape((images.shape[0], -1))  # Flatten images\n",
        "    distances = pdist(images_flat, metric='euclidean')\n",
        "    diversity = np.mean(distances)\n",
        "    return diversity\n",
        "\n",
        "diversity_score = measure_diversity(binary_generated_samples)\n",
        "print(f\"Diversity Score: {diversity_score}\")\n",
        "\n",
        "def measure_coverage(real_images, generated_images, threshold=0.5):\n",
        "    real_images_flat = real_images.reshape((real_images.shape[0], -1))  # Flatten images\n",
        "    generated_images_flat = generated_images.reshape((generated_images.shape[0], -1))  # Flatten images\n",
        "\n",
        "    covered = 0\n",
        "    for real_image in real_images_flat:\n",
        "        distances = np.linalg.norm(generated_images_flat - real_image, axis=1)\n",
        "        if np.any(distances < threshold):\n",
        "            covered += 1\n",
        "\n",
        "    coverage = covered / real_images_flat.shape[0]\n",
        "    return coverage\n",
        "\n",
        "coverage_score = measure_coverage(binary_test_data, binary_generated_samples)\n",
        "print(f\"Coverage Score: {coverage_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeFMYdPIHLVU",
        "outputId": "c86a6dc6-82b0-4293-8290-ed4ff3d5f4f0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num real: 304 Num fake: 304\n",
            "Precision: 0.9210526315789473\n",
            "Recall: 0.003289473684210526\n",
            "Diversity Score: 3.3410601822311667\n",
            "Coverage Score: 0.0\n"
          ]
        }
      ]
    }
  ]
}